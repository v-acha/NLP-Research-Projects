{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","toc_visible":true,"authorship_tag":"ABX9TyMZ9R0+VlA7T+xMIxeouEmU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"f78f7dd41b5248c088278a2771151db5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0b5356ab2edf40f49cf982d635ccac73","IPY_MODEL_9e0111788ea94063a46511550ccd8cb8","IPY_MODEL_6544fee776484719a12b406905579a7a"],"layout":"IPY_MODEL_e7309129d9584721bf758552aa373c82"}},"0b5356ab2edf40f49cf982d635ccac73":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad9379974ca34b63a90560a3f28880af","placeholder":"​","style":"IPY_MODEL_82aa7625666e40b6a70a89c0b50619b5","value":"tokenizer_config.json: 100%"}},"9e0111788ea94063a46511550ccd8cb8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f35fa769ef24535a6451ec2acd8bfb4","max":2537,"min":0,"orientation":"horizontal","style":"IPY_MODEL_be543219dafe4f62a630053d4abf1475","value":2537}},"6544fee776484719a12b406905579a7a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ad117d0f3d141d7a8f38bfdc9f63a8b","placeholder":"​","style":"IPY_MODEL_bbb3f70bd3834c7393e61973f06f848e","value":" 2.54k/2.54k [00:00&lt;00:00, 315kB/s]"}},"e7309129d9584721bf758552aa373c82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad9379974ca34b63a90560a3f28880af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82aa7625666e40b6a70a89c0b50619b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f35fa769ef24535a6451ec2acd8bfb4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be543219dafe4f62a630053d4abf1475":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8ad117d0f3d141d7a8f38bfdc9f63a8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbb3f70bd3834c7393e61973f06f848e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"36f2c492039240e3b7707b87c65ae587":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d0c19bfd3cbd48aa95f3b9c97cc8e246","IPY_MODEL_bff48b0585e64518bc89e5302fa63072","IPY_MODEL_646a47858b7a4407ab539d2d610491c0"],"layout":"IPY_MODEL_9ce4145d90fd4722b7ca75fd43766b55"}},"d0c19bfd3cbd48aa95f3b9c97cc8e246":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db0d9bf4923b409e92efeac8f5d12549","placeholder":"​","style":"IPY_MODEL_8f142696ce7b449daad1c6ff3fa3dd8f","value":"spiece.model: 100%"}},"bff48b0585e64518bc89e5302fa63072":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac57f5765e4f41adba08deeb3f9e4600","max":791656,"min":0,"orientation":"horizontal","style":"IPY_MODEL_24c67a93107a47609595dfb55d12d0de","value":791656}},"646a47858b7a4407ab539d2d610491c0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3687757ac9942518dc4a0afa937532f","placeholder":"​","style":"IPY_MODEL_7bfff0db30614170a72e617e8226d05f","value":" 792k/792k [00:00&lt;00:00, 5.69MB/s]"}},"9ce4145d90fd4722b7ca75fd43766b55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db0d9bf4923b409e92efeac8f5d12549":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f142696ce7b449daad1c6ff3fa3dd8f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac57f5765e4f41adba08deeb3f9e4600":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24c67a93107a47609595dfb55d12d0de":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b3687757ac9942518dc4a0afa937532f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7bfff0db30614170a72e617e8226d05f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"421fbbda2e024ec8b881969f3d801fb9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3362e62553b0451faed3f1cf0b6510d3","IPY_MODEL_741e9ab2708d43b993eed08f837bdf68","IPY_MODEL_29b387c470b5410a849588295c9c2460"],"layout":"IPY_MODEL_0a9b6733c9e04751ab0c2dfd4f5bb78c"}},"3362e62553b0451faed3f1cf0b6510d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c23875762bb04470ae3e0e201b4eb425","placeholder":"​","style":"IPY_MODEL_ea32f77d813440aeb427db76d114b8d4","value":"tokenizer.json: 100%"}},"741e9ab2708d43b993eed08f837bdf68":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_57f8bb26325a4bcbaf0ee66cdc101a24","max":2424064,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8ecd5761dbbf41c3a4a8134048ef8987","value":2424064}},"29b387c470b5410a849588295c9c2460":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5b65d083bb747f9b0a7a93793483335","placeholder":"​","style":"IPY_MODEL_4af017e5e85844f4a26a92b4f42c347b","value":" 2.42M/2.42M [00:00&lt;00:00, 26.4MB/s]"}},"0a9b6733c9e04751ab0c2dfd4f5bb78c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c23875762bb04470ae3e0e201b4eb425":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea32f77d813440aeb427db76d114b8d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57f8bb26325a4bcbaf0ee66cdc101a24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ecd5761dbbf41c3a4a8134048ef8987":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f5b65d083bb747f9b0a7a93793483335":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4af017e5e85844f4a26a92b4f42c347b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"feb644518026408f8b55bd9125363364":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_315bc2461d7f4ed4830be48abf05b530","IPY_MODEL_c4fbda37bd2d4c9692a7675fc0ee397f","IPY_MODEL_45c9a336702c485ca1c1713edc4c1053"],"layout":"IPY_MODEL_da6fa4366d5b4e66b2c09831607e8196"}},"315bc2461d7f4ed4830be48abf05b530":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d0ee5c146ad4be8973dc664edce4455","placeholder":"​","style":"IPY_MODEL_73a4b016f97a414d9215f2e8fc4499e9","value":"special_tokens_map.json: 100%"}},"c4fbda37bd2d4c9692a7675fc0ee397f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5207e12e6c5b441eb96abfc889581d99","max":2201,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6cb1d42353ea4297923713c71ca6ab45","value":2201}},"45c9a336702c485ca1c1713edc4c1053":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c68e875f88e040d7b3252fc4cf632281","placeholder":"​","style":"IPY_MODEL_7623dffb25d545e58f537a5b896ebfda","value":" 2.20k/2.20k [00:00&lt;00:00, 304kB/s]"}},"da6fa4366d5b4e66b2c09831607e8196":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d0ee5c146ad4be8973dc664edce4455":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73a4b016f97a414d9215f2e8fc4499e9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5207e12e6c5b441eb96abfc889581d99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cb1d42353ea4297923713c71ca6ab45":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c68e875f88e040d7b3252fc4cf632281":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7623dffb25d545e58f537a5b896ebfda":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93fea42357544e47a136bc31ec13c7d1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4ce4f95acd12400abb06eb7d43717afb","IPY_MODEL_e7f0cb34f94549379772ef5978ff1dfd","IPY_MODEL_8f5e6297ebff4a05b895feaa60ea8bae"],"layout":"IPY_MODEL_d8cd1323f2144fb3820dcee77356087a"}},"4ce4f95acd12400abb06eb7d43717afb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6361f5595d284d30a871c4115353fa9b","placeholder":"​","style":"IPY_MODEL_12101508684c4e2eb84236c6a3f8d1a8","value":"config.json: 100%"}},"e7f0cb34f94549379772ef5978ff1dfd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a8b34080bde4c2c930e6a0f852e7ae1","max":1404,"min":0,"orientation":"horizontal","style":"IPY_MODEL_80cd79da9e334f81bb551ec0797afb12","value":1404}},"8f5e6297ebff4a05b895feaa60ea8bae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5459fcedaf49415c8a3bb74f8fe3cb00","placeholder":"​","style":"IPY_MODEL_e420e2a92a9a492184c1edbc55c86ee4","value":" 1.40k/1.40k [00:00&lt;00:00, 176kB/s]"}},"d8cd1323f2144fb3820dcee77356087a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6361f5595d284d30a871c4115353fa9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12101508684c4e2eb84236c6a3f8d1a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0a8b34080bde4c2c930e6a0f852e7ae1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80cd79da9e334f81bb551ec0797afb12":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5459fcedaf49415c8a3bb74f8fe3cb00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e420e2a92a9a492184c1edbc55c86ee4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d27c58064d8462baaaa52b7017ba5cd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d5e43d1f817143b8be44021800548b48","IPY_MODEL_3b0d84bdd2954655a8c571073858e37d","IPY_MODEL_bebe7212fd514b0bb474e9a6790bbffd"],"layout":"IPY_MODEL_004307e3de4540c9a59ce778dd6dade1"}},"d5e43d1f817143b8be44021800548b48":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a9895c8472542c399475a88468b9d8b","placeholder":"​","style":"IPY_MODEL_a4012d89ab8f427292fe85277c658764","value":"model.safetensors: 100%"}},"3b0d84bdd2954655a8c571073858e37d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_15b61dc47f384370b3023ce80d332640","max":990345061,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4f381758feed4ee38b8fd06942212007","value":990345061}},"bebe7212fd514b0bb474e9a6790bbffd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f2bd97b977344d89b4ee9e0e68226de","placeholder":"​","style":"IPY_MODEL_9f89addd999143debdd3305adf2dc3f8","value":" 990M/990M [00:07&lt;00:00, 140MB/s]"}},"004307e3de4540c9a59ce778dd6dade1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a9895c8472542c399475a88468b9d8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4012d89ab8f427292fe85277c658764":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"15b61dc47f384370b3023ce80d332640":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f381758feed4ee38b8fd06942212007":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8f2bd97b977344d89b4ee9e0e68226de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f89addd999143debdd3305adf2dc3f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ab8895430d7b42809193aa020b6409b3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c996842f703e491fbf7643258cecf715","IPY_MODEL_655d14060e394a549fde9d8b636bee36","IPY_MODEL_dc1da0fba067471b8e7b038abca70a4a"],"layout":"IPY_MODEL_931524707801409d83cc69966d4289fb"}},"c996842f703e491fbf7643258cecf715":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5952bf5dad1d4d258fb084a0d9f60d0c","placeholder":"​","style":"IPY_MODEL_1ecece40635d495f9206dbdcd18d8abe","value":"generation_config.json: 100%"}},"655d14060e394a549fde9d8b636bee36":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_071b9092b2d54c80be982a3b370fd935","max":147,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2b95a9b00772416eb5cb0fbf49138ae0","value":147}},"dc1da0fba067471b8e7b038abca70a4a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_80086a55c41a45a4b0ecffbb02cddf1f","placeholder":"​","style":"IPY_MODEL_f164361de3924b6c84de6c5bd3ae4f58","value":" 147/147 [00:00&lt;00:00, 19.1kB/s]"}},"931524707801409d83cc69966d4289fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5952bf5dad1d4d258fb084a0d9f60d0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ecece40635d495f9206dbdcd18d8abe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"071b9092b2d54c80be982a3b370fd935":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b95a9b00772416eb5cb0fbf49138ae0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"80086a55c41a45a4b0ecffbb02cddf1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f164361de3924b6c84de6c5bd3ae4f58":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41d4a5b0ee194739ab5fb4ca1b35c71f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6bc3d142793d42d49bfde1c239bd7c36","IPY_MODEL_32b6eeb630284259abe7827e7ff9e837","IPY_MODEL_ab63d8796f3d49b8a991ec1f1a4593f7"],"layout":"IPY_MODEL_7e23580c91a7477ba3ab58c7d55b801e"}},"6bc3d142793d42d49bfde1c239bd7c36":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_83de8d10914b45e18674ccf157f18bab","placeholder":"​","style":"IPY_MODEL_676d99b7ec794bbaba4a8b235ee8c065","value":"Map: 100%"}},"32b6eeb630284259abe7827e7ff9e837":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a89824b2d0f4d3a80f0e2154bd3be5c","max":351082,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6bf3119bd2bf4e41b4fd002e73ee38e6","value":351082}},"ab63d8796f3d49b8a991ec1f1a4593f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69007777636849219692c85ac932575b","placeholder":"​","style":"IPY_MODEL_6303fdbd79b047c58e9bde132bdd0145","value":" 351082/351082 [02:44&lt;00:00, 2040.22 examples/s]"}},"7e23580c91a7477ba3ab58c7d55b801e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83de8d10914b45e18674ccf157f18bab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"676d99b7ec794bbaba4a8b235ee8c065":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a89824b2d0f4d3a80f0e2154bd3be5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bf3119bd2bf4e41b4fd002e73ee38e6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"69007777636849219692c85ac932575b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6303fdbd79b047c58e9bde132bdd0145":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff0a11fb553a4b7eb0b1780940ff6dd5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b778dc820366422480164a730d6a96b2","IPY_MODEL_3c0a6bb9ead041b989d6119156490bee","IPY_MODEL_632db0d3950b40278715a4333393c739"],"layout":"IPY_MODEL_5257c88b8c0d4b3794cec13be1b3d85f"}},"b778dc820366422480164a730d6a96b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4f57763892d41418a2a644f585fef6d","placeholder":"​","style":"IPY_MODEL_56658446bd494375b3f0443618a337b2","value":"Map: 100%"}},"3c0a6bb9ead041b989d6119156490bee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1025d5740324c33bdce516b04f4d6c7","max":43885,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4afc11802b324f0480e5a38f90a4e9df","value":43885}},"632db0d3950b40278715a4333393c739":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f01d66233317468382dff61898162ba7","placeholder":"​","style":"IPY_MODEL_9454b2f3f55f46648c59c71700a693a3","value":" 43885/43885 [00:25&lt;00:00, 2221.84 examples/s]"}},"5257c88b8c0d4b3794cec13be1b3d85f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4f57763892d41418a2a644f585fef6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56658446bd494375b3f0443618a337b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a1025d5740324c33bdce516b04f4d6c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4afc11802b324f0480e5a38f90a4e9df":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f01d66233317468382dff61898162ba7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9454b2f3f55f46648c59c71700a693a3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dfc13dc94eb046009ed56e0ce939c452":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cc3d2969cf21477d8520ca9fb8612b40","IPY_MODEL_c405b84b5daa473c8f74a1e8c7c0fcce","IPY_MODEL_1d7d7710937e4b0093c69a924f2246db"],"layout":"IPY_MODEL_afa8e366a6174c36872ad2e982af92d0"}},"cc3d2969cf21477d8520ca9fb8612b40":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d00eefac8f5041be940ce1a99ef6b00d","placeholder":"​","style":"IPY_MODEL_3416aaca0f11441c9ba83dff72010572","value":"Map: 100%"}},"c405b84b5daa473c8f74a1e8c7c0fcce":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9c0e9bae96e46bea707ec23e617538a","max":43886,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b45ed78e54aa4cdda33250ec70b3aa5e","value":43886}},"1d7d7710937e4b0093c69a924f2246db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4846fa4507454a87b68bc7bb6247359c","placeholder":"​","style":"IPY_MODEL_da42e9578b6a4b569b430ed5dbbbecb3","value":" 43886/43886 [00:21&lt;00:00, 2152.52 examples/s]"}},"afa8e366a6174c36872ad2e982af92d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d00eefac8f5041be940ce1a99ef6b00d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3416aaca0f11441c9ba83dff72010572":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a9c0e9bae96e46bea707ec23e617538a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b45ed78e54aa4cdda33250ec70b3aa5e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4846fa4507454a87b68bc7bb6247359c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da42e9578b6a4b569b430ed5dbbbecb3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Experiment 1: Train Flan 5 on the chunked dataset first.\n","Since we already have the tokenized dataset that was used for our t5 baseline model 2, we will proceed to use that dataset and fine-tune only the last four layers, allowing the model to specialize in legal text patterns without excessive training costs.\n","\n","Then we will move on to experiment with more detailed prompts, intelligent context based chunkings, hierarchical retrieval and more layers freezing or unfreezing"],"metadata":{"id":"IeDUudqnWxrk"}},{"cell_type":"code","source":["!pip install -q transformers\n","!pip install -q datasets\n","!pip install -q evaluate\n","!pip install -q tokenizers\n","!pip install -q torch\n","!pip install -q evaluate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"91fRhsLSNpsx","executionInfo":{"status":"ok","timestamp":1742230674427,"user_tz":240,"elapsed":88838,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}},"outputId":"50e88aa0-daaa-41a9-b53e-6e0853699c7a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/487.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m430.1/487.4 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["! pip install -q accelerate"],"metadata":{"id":"blY5VTlxOeB2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mount Google Drive to access the saved dataset\n","from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rlBOOHcZOVmR","executionInfo":{"status":"ok","timestamp":1742230713888,"user_tz":240,"elapsed":14973,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}},"outputId":"84e18d9c-78ca-4a24-8257-2f1d1b53e67b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Experiment 1: Finetuning Flan t5 on previous tokenized chunked window sliding (128 window sliding) dataset.\n","\n","Since we already have the tokenized dataset that was used for our t5 baseline model 2, we will proceed to use that dataset and fine-tune only the last four layers, allowing the model to specialize in legal text patterns without excessive training costs."],"metadata":{"id":"62in1R_TX--4"}},{"cell_type":"markdown","source":["Step 1: Setting Up the Environment in Google Colab. Loading our tokenized chunked dataset that was used to train the t5 baseline"],"metadata":{"id":"ZP7exUwROvXY"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cor8_uo6M5tH","executionInfo":{"status":"ok","timestamp":1742230907508,"user_tz":240,"elapsed":32732,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}},"outputId":"da79e121-5a1d-4f6e-c20a-2b19fe2fd0fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","DatasetDict({\n","    train: Dataset({\n","        features: ['doc_id', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 351082\n","    })\n","    validation: Dataset({\n","        features: ['doc_id', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 43885\n","    })\n","    test: Dataset({\n","        features: ['doc_id', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 43886\n","    })\n","})\n"]}],"source":["# Check and enable GPU\n","import torch\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")\n","\n","# Load the Hugging Face dataset\n","from datasets import load_from_disk\n","\n","dataset_path = \"/content/drive/My Drive/Colab Notebooks/NLP_266_Project/chunked_datasets\"\n","tokenized_dataset = load_from_disk(dataset_path)\n","\n","# Confirm dataset structure\n","print(tokenized_dataset)\n"]},{"cell_type":"markdown","source":["Step 2: Load FLAN-T5 Model and Tokenizer"],"metadata":{"id":"hK99oE3eO8oC"}},{"cell_type":"code","source":["from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n","\n","# Load FLAN-T5 model and tokenizer (base version to start)\n","model_name = \"google/flan-t5-base\"  # Change to \"flan-t5-large\" if needed\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# Load model and move to GPU\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","model.to(device)\n","\n","# Verify model structure\n","print(model)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["f78f7dd41b5248c088278a2771151db5","0b5356ab2edf40f49cf982d635ccac73","9e0111788ea94063a46511550ccd8cb8","6544fee776484719a12b406905579a7a","e7309129d9584721bf758552aa373c82","ad9379974ca34b63a90560a3f28880af","82aa7625666e40b6a70a89c0b50619b5","2f35fa769ef24535a6451ec2acd8bfb4","be543219dafe4f62a630053d4abf1475","8ad117d0f3d141d7a8f38bfdc9f63a8b","bbb3f70bd3834c7393e61973f06f848e","36f2c492039240e3b7707b87c65ae587","d0c19bfd3cbd48aa95f3b9c97cc8e246","bff48b0585e64518bc89e5302fa63072","646a47858b7a4407ab539d2d610491c0","9ce4145d90fd4722b7ca75fd43766b55","db0d9bf4923b409e92efeac8f5d12549","8f142696ce7b449daad1c6ff3fa3dd8f","ac57f5765e4f41adba08deeb3f9e4600","24c67a93107a47609595dfb55d12d0de","b3687757ac9942518dc4a0afa937532f","7bfff0db30614170a72e617e8226d05f","421fbbda2e024ec8b881969f3d801fb9","3362e62553b0451faed3f1cf0b6510d3","741e9ab2708d43b993eed08f837bdf68","29b387c470b5410a849588295c9c2460","0a9b6733c9e04751ab0c2dfd4f5bb78c","c23875762bb04470ae3e0e201b4eb425","ea32f77d813440aeb427db76d114b8d4","57f8bb26325a4bcbaf0ee66cdc101a24","8ecd5761dbbf41c3a4a8134048ef8987","f5b65d083bb747f9b0a7a93793483335","4af017e5e85844f4a26a92b4f42c347b","feb644518026408f8b55bd9125363364","315bc2461d7f4ed4830be48abf05b530","c4fbda37bd2d4c9692a7675fc0ee397f","45c9a336702c485ca1c1713edc4c1053","da6fa4366d5b4e66b2c09831607e8196","7d0ee5c146ad4be8973dc664edce4455","73a4b016f97a414d9215f2e8fc4499e9","5207e12e6c5b441eb96abfc889581d99","6cb1d42353ea4297923713c71ca6ab45","c68e875f88e040d7b3252fc4cf632281","7623dffb25d545e58f537a5b896ebfda","93fea42357544e47a136bc31ec13c7d1","4ce4f95acd12400abb06eb7d43717afb","e7f0cb34f94549379772ef5978ff1dfd","8f5e6297ebff4a05b895feaa60ea8bae","d8cd1323f2144fb3820dcee77356087a","6361f5595d284d30a871c4115353fa9b","12101508684c4e2eb84236c6a3f8d1a8","0a8b34080bde4c2c930e6a0f852e7ae1","80cd79da9e334f81bb551ec0797afb12","5459fcedaf49415c8a3bb74f8fe3cb00","e420e2a92a9a492184c1edbc55c86ee4","3d27c58064d8462baaaa52b7017ba5cd","d5e43d1f817143b8be44021800548b48","3b0d84bdd2954655a8c571073858e37d","bebe7212fd514b0bb474e9a6790bbffd","004307e3de4540c9a59ce778dd6dade1","0a9895c8472542c399475a88468b9d8b","a4012d89ab8f427292fe85277c658764","15b61dc47f384370b3023ce80d332640","4f381758feed4ee38b8fd06942212007","8f2bd97b977344d89b4ee9e0e68226de","9f89addd999143debdd3305adf2dc3f8","ab8895430d7b42809193aa020b6409b3","c996842f703e491fbf7643258cecf715","655d14060e394a549fde9d8b636bee36","dc1da0fba067471b8e7b038abca70a4a","931524707801409d83cc69966d4289fb","5952bf5dad1d4d258fb084a0d9f60d0c","1ecece40635d495f9206dbdcd18d8abe","071b9092b2d54c80be982a3b370fd935","2b95a9b00772416eb5cb0fbf49138ae0","80086a55c41a45a4b0ecffbb02cddf1f","f164361de3924b6c84de6c5bd3ae4f58"]},"id":"unhdHDK3O9Rf","executionInfo":{"status":"ok","timestamp":1742230934422,"user_tz":240,"elapsed":21897,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}},"outputId":"7da8e909-e875-4075-e600-67f55efb5b96"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f78f7dd41b5248c088278a2771151db5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36f2c492039240e3b7707b87c65ae587"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"421fbbda2e024ec8b881969f3d801fb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"feb644518026408f8b55bd9125363364"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93fea42357544e47a136bc31ec13c7d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d27c58064d8462baaaa52b7017ba5cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab8895430d7b42809193aa020b6409b3"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["T5ForConditionalGeneration(\n","  (shared): Embedding(32128, 768)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-11): 11 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-11): 11 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",")\n"]}]},{"cell_type":"code","source":["print(model.decoder.block[-4:])  # Access the last 4 decoder layers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zIaW8EnOQe1q","executionInfo":{"status":"ok","timestamp":1742230942724,"user_tz":240,"elapsed":5,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}},"outputId":"be575a9d-f588-4892-8b42-a04876672e96"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ModuleList(\n","  (0-3): 4 x T5Block(\n","    (layer): ModuleList(\n","      (0): T5LayerSelfAttention(\n","        (SelfAttention): T5Attention(\n","          (q): Linear(in_features=768, out_features=768, bias=False)\n","          (k): Linear(in_features=768, out_features=768, bias=False)\n","          (v): Linear(in_features=768, out_features=768, bias=False)\n","          (o): Linear(in_features=768, out_features=768, bias=False)\n","        )\n","        (layer_norm): T5LayerNorm()\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): T5LayerCrossAttention(\n","        (EncDecAttention): T5Attention(\n","          (q): Linear(in_features=768, out_features=768, bias=False)\n","          (k): Linear(in_features=768, out_features=768, bias=False)\n","          (v): Linear(in_features=768, out_features=768, bias=False)\n","          (o): Linear(in_features=768, out_features=768, bias=False)\n","        )\n","        (layer_norm): T5LayerNorm()\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): T5LayerFF(\n","        (DenseReluDense): T5DenseGatedActDense(\n","          (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","          (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","          (wo): Linear(in_features=2048, out_features=768, bias=False)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (act): NewGELUActivation()\n","        )\n","        (layer_norm): T5LayerNorm()\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","  )\n",")\n"]}]},{"cell_type":"markdown","source":["Step 3: Freeze Lower Transformer Layers\n","To efficiently fine-tune FLAN-T5, we will freeze all but the last four layers. This prevents unnecessary updates to general pre-trained knowledge while allowing specialization for contract-specific clause extraction."],"metadata":{"id":"OOSO8BPvQK5o"}},{"cell_type":"code","source":["# Freeze all layers except the last 4\n","for param in model.parameters():\n","    param.requires_grad = False  # Freeze all layers\n","\n","# Unfreeze the last 4 transformer layers\n","num_layers_to_unfreeze = 4  # Adjust as needed\n","for layer in model.encoder.block[-num_layers_to_unfreeze:]:  # Unfreeze last N layers\n","    for param in layer.parameters():\n","        param.requires_grad = True\n","\n","# Confirm frozen layers\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"Trainable parameters: {trainable_params}/{total_params}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lZTJ5Dq8QLw3","executionInfo":{"status":"ok","timestamp":1742230948873,"user_tz":240,"elapsed":6,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}},"outputId":"cd1cf345-7c82-408e-82c5-46c3546b4d41"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Trainable parameters: 28317696/247577856\n"]}]},{"cell_type":"markdown","source":["#### Double Check Model Freezing\n","Since you are fine-tuning only the last few layers, make sure not all layers are frozen.\n","Try printing trainable layers:"],"metadata":{"id":"gIlTP7f4EE4J"}},{"cell_type":"code","source":["for name, param in model.named_parameters():\n","    if param.requires_grad:\n","        print(f\"Trainable: {name}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KqmgsLQkD3sw","executionInfo":{"status":"ok","timestamp":1742230966505,"user_tz":240,"elapsed":6,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}},"outputId":"d27900d1-604c-4489-fbc4-c0dce73f381a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Trainable: encoder.block.8.layer.0.SelfAttention.q.weight\n","Trainable: encoder.block.8.layer.0.SelfAttention.k.weight\n","Trainable: encoder.block.8.layer.0.SelfAttention.v.weight\n","Trainable: encoder.block.8.layer.0.SelfAttention.o.weight\n","Trainable: encoder.block.8.layer.0.layer_norm.weight\n","Trainable: encoder.block.8.layer.1.DenseReluDense.wi_0.weight\n","Trainable: encoder.block.8.layer.1.DenseReluDense.wi_1.weight\n","Trainable: encoder.block.8.layer.1.DenseReluDense.wo.weight\n","Trainable: encoder.block.8.layer.1.layer_norm.weight\n","Trainable: encoder.block.9.layer.0.SelfAttention.q.weight\n","Trainable: encoder.block.9.layer.0.SelfAttention.k.weight\n","Trainable: encoder.block.9.layer.0.SelfAttention.v.weight\n","Trainable: encoder.block.9.layer.0.SelfAttention.o.weight\n","Trainable: encoder.block.9.layer.0.layer_norm.weight\n","Trainable: encoder.block.9.layer.1.DenseReluDense.wi_0.weight\n","Trainable: encoder.block.9.layer.1.DenseReluDense.wi_1.weight\n","Trainable: encoder.block.9.layer.1.DenseReluDense.wo.weight\n","Trainable: encoder.block.9.layer.1.layer_norm.weight\n","Trainable: encoder.block.10.layer.0.SelfAttention.q.weight\n","Trainable: encoder.block.10.layer.0.SelfAttention.k.weight\n","Trainable: encoder.block.10.layer.0.SelfAttention.v.weight\n","Trainable: encoder.block.10.layer.0.SelfAttention.o.weight\n","Trainable: encoder.block.10.layer.0.layer_norm.weight\n","Trainable: encoder.block.10.layer.1.DenseReluDense.wi_0.weight\n","Trainable: encoder.block.10.layer.1.DenseReluDense.wi_1.weight\n","Trainable: encoder.block.10.layer.1.DenseReluDense.wo.weight\n","Trainable: encoder.block.10.layer.1.layer_norm.weight\n","Trainable: encoder.block.11.layer.0.SelfAttention.q.weight\n","Trainable: encoder.block.11.layer.0.SelfAttention.k.weight\n","Trainable: encoder.block.11.layer.0.SelfAttention.v.weight\n","Trainable: encoder.block.11.layer.0.SelfAttention.o.weight\n","Trainable: encoder.block.11.layer.0.layer_norm.weight\n","Trainable: encoder.block.11.layer.1.DenseReluDense.wi_0.weight\n","Trainable: encoder.block.11.layer.1.DenseReluDense.wi_1.weight\n","Trainable: encoder.block.11.layer.1.DenseReluDense.wo.weight\n","Trainable: encoder.block.11.layer.1.layer_norm.weight\n"]}]},{"cell_type":"markdown","source":["Step 4: Define Fine-Tuning Parameters"],"metadata":{"id":"dLh8BiwKRk27"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Check if any NaN values exist in labels\n","has_nan = any(np.isnan(sample[\"labels\"]).any() for sample in tokenized_dataset[\"train\"])\n","print(\"Any NaN values in dataset?\", has_nan)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U9wTe3K-FAJY","executionInfo":{"status":"ok","timestamp":1742231467197,"user_tz":240,"elapsed":155324,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}},"outputId":"4f3c0c42-3c08-4e55-c6cb-25e579a9c179"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-14-4472cc3a0418>:4: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n","  has_nan = any(np.isnan(sample[\"labels\"]).any() for sample in tokenized_dataset[\"train\"])\n"]},{"output_type":"stream","name":"stdout","text":["Any NaN values in dataset? False\n"]}]},{"cell_type":"code","source":["from transformers import TrainingArguments\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./flan_t5_finetuned\",\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    logging_strategy=\"steps\",\n","    learning_rate=3e-5,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    weight_decay=0.01,\n","    save_total_limit=2,\n","    num_train_epochs=2,\n","    fp16=True,  # Enable mixed precision training\n","    logging_dir=\"./logs\",\n","    logging_steps=500,\n","    report_to=\"none\",\n","    gradient_accumulation_steps=2,  # Helps prevent NaN loss on small batch sizes\n","    max_grad_norm=1.0  # Clipping gradients prevents exploding updates\n",")\n","\n","# Print training arguments\n","print(training_args)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m4vi6DwrRl0K","executionInfo":{"status":"ok","timestamp":1742233466051,"user_tz":240,"elapsed":56,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}},"outputId":"d991ebd5-7a4b-4dc1-fb91-4118e6d808ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TrainingArguments(\n","_n_gpu=1,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=False,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=IntervalStrategy.EPOCH,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=True,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=2,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=3e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=./logs,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=2,\n","optim=OptimizerNames.ADAMW_TORCH,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=./flan_t5_finetuned,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=16,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=[],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=./flan_t5_finetuned,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=SaveStrategy.EPOCH,\n","save_total_limit=2,\n","seed=42,\n","skip_memory_metrics=True,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.01,\n",")\n"]}]},{"cell_type":"markdown","source":["Step 5: Prepare Dataset for FLAN-T5 Fine-Tuning"],"metadata":{"id":"l6sCf714R72F"}},{"cell_type":"markdown","source":["Our dataset likely contains labels that were tokenized in a batched manner, leading to lists of NumPy arrays instead of single NumPy arrays. It ensures labels are stored efficiently before set_format(\"torch\") is applied.\n","Prevents inefficient tensor creation that triggers PyTorch warnings"],"metadata":{"id":"qGEXqSLtCtnb"}},{"cell_type":"code","source":["from transformers import DataCollatorForSeq2Seq\n","import numpy as np\n","from transformers import DataCollatorForSeq2Seq\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, label_pad_token_id=-100,pad_to_multiple_of=8)\n","\n","# Ensure all labels are NumPy arrays before converting to PyTorch tensors\n","def normalize_labels(dataset):\n","    dataset = dataset.map(lambda x: {\"labels\": np.array(x[\"labels\"], dtype=np.int64)})\n","    return dataset\n","\n","tokenized_dataset = normalize_labels(tokenized_dataset)\n","tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"doc_id\"])\n","\n","# Check a sample\n","print(tokenized_dataset[\"train\"][0])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["41d4a5b0ee194739ab5fb4ca1b35c71f","6bc3d142793d42d49bfde1c239bd7c36","32b6eeb630284259abe7827e7ff9e837","ab63d8796f3d49b8a991ec1f1a4593f7","7e23580c91a7477ba3ab58c7d55b801e","83de8d10914b45e18674ccf157f18bab","676d99b7ec794bbaba4a8b235ee8c065","7a89824b2d0f4d3a80f0e2154bd3be5c","6bf3119bd2bf4e41b4fd002e73ee38e6","69007777636849219692c85ac932575b","6303fdbd79b047c58e9bde132bdd0145","ff0a11fb553a4b7eb0b1780940ff6dd5","b778dc820366422480164a730d6a96b2","3c0a6bb9ead041b989d6119156490bee","632db0d3950b40278715a4333393c739","5257c88b8c0d4b3794cec13be1b3d85f","b4f57763892d41418a2a644f585fef6d","56658446bd494375b3f0443618a337b2","a1025d5740324c33bdce516b04f4d6c7","4afc11802b324f0480e5a38f90a4e9df","f01d66233317468382dff61898162ba7","9454b2f3f55f46648c59c71700a693a3","dfc13dc94eb046009ed56e0ce939c452","cc3d2969cf21477d8520ca9fb8612b40","c405b84b5daa473c8f74a1e8c7c0fcce","1d7d7710937e4b0093c69a924f2246db","afa8e366a6174c36872ad2e982af92d0","d00eefac8f5041be940ce1a99ef6b00d","3416aaca0f11441c9ba83dff72010572","a9c0e9bae96e46bea707ec23e617538a","b45ed78e54aa4cdda33250ec70b3aa5e","4846fa4507454a87b68bc7bb6247359c","da42e9578b6a4b569b430ed5dbbbecb3"]},"id":"EipWlSquR9ur","executionInfo":{"status":"ok","timestamp":1742233392845,"user_tz":240,"elapsed":211735,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}},"outputId":"5f47d6b0-50d3-42e9-8b47-ed92891ca32b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/351082 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41d4a5b0ee194739ab5fb4ca1b35c71f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/43885 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff0a11fb553a4b7eb0b1780940ff6dd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/43886 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfc13dc94eb046009ed56e0ce939c452"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'doc_id': tensor(10270), 'input_ids': tensor([   13,  3501,  1112,   831,    57,  7327, 22871,   282,     7,   343,\n","        11899, 19674,     6,   276,  9164,     7,    11, 18892,   937,    57,\n","         7327,    12,  2024,  3501,   982,     6,    38,  1316,     6,   441,\n","            8,  4732,    13,     8,  2786,  5968,  4330, 25266,     7, 22871,\n","        11985, 20230,  4300,  4012,  1576, 15694,    21,    46,   641,  3029,\n","          988,    13,   284,  3501,   556,   338,     8, 11045,  1068,   483,\n","          610,  4293, 13308,  1109,   127,  1576,  5941,   121,    19,   136,\n","          483,    16,     8,  1576,   381,    12,     8,   269,    13,     8,\n","          500,    57,     8,   889,  4818,     3,    18,     3,    15,     5,\n","          122,     5,    10,     3,   226,     5,    63,    61, 22871,  7740,\n","          136,  1081, 13320,    21,   917,   889,  1081,    11,    66,  7185,\n","         3864, 22871, 11985,   917,   889,  5298,  1112,     6, 15694,    11,\n","        18892,     6,    11,   136,  3864,    12,  7185,     5,   933,  4921,\n","        11045,    13,   136,   831,  3501,  5298,  1112,   906,    38,     3,\n","            9,   741,    13,     8,  1112,     6, 15694,    11, 18892,     6,\n","           11,    87,   127,  3864, 22871, 11985,   358,   889,  5298,  1112,\n","            6, 15694,    11, 18892,     6,    11,  3864,     5, 22871, 21582,\n","          136,   831,  3501,  5298,  1112,     3,   390,    30,  7327,    31,\n","            7,  5719, 22871,  7740,    66,   414,  1139,   380,   379,  3602,\n","           13,   917,  1139,    27,    26,     7,    11,  1637,    11,   414,\n","         1139,  4735, 12262,     7, 22871,  7740,  1502,    21, 31858,     6,\n","        11981,    11,    87,   127,     3, 15917,    53,    13,   136,   917,\n","           11,  3501,  4303,  2073, 22871, 21582,   136,   831, 31858,     6,\n","        11981,    11,    87,   127,     3, 15917,    53,    13,   136,   917,\n","           11,  3501,  4303,  2073, 22871,  7740,  7192,    21,   821,  1341,\n","        15577,    87,  1018, 26703,  3803, 22871, 21582,  2332,  3501,   821,\n","        23668,    11,   370,  1151,   821, 23668,     3,   390,    30,  1693,\n","           13,   821, 15905,   937,    57,  7327, 22871,   282,     7,   343,\n","         7740,  7192,    30,   821, 15905,    11,   821,   772,    24,     8,\n","          917,    11,  3501,   225,    36,  8413,   581,     6,   379,   136,\n","         1316,  1339,    11,    87,   127,  4943,     7,    12,  2868,   821,\n","          331, 22871, 21582, 10069,   821,  4891,    11,   331,  1232, 22871,\n","          282,     7,   343, 15192, 20230,  4300,  3379,    21,  5576,    11,\n","           87,   127,    70,  3053,    16,  2594,    41,   413,    87,  3035,\n","         2637,    61, 22871,   282,     7,   343,  7327,    16,  2505,  2231,\n","           11,  4891,    13,  7327,    18, 29189,    26,   821,  1339,    11,\n","        15905,    12,  2082,     3,    99,  1164,    19,  3085, 14007, 22871,\n","          282,     7,   343,  6767,    13,   821,    11,  2614,   331,    38,\n","          937,    57,  7327,    31,     7,   917,   599,     7,    61,   889,\n","          282,     7,   343, 22871, 21582,   821, 15220,     7,    12,     8,\n","          806, 20230,  4300,     3,   390,    30, 15552, 23668,  8755,    11,\n","         4293,  8794,    57,  7327,    11,   806,    12,  7327,   917,   599,\n","            7,    61, 22871,  6767,    13,   821,    11,  2614,   331,    38,\n","          937,    57,  1002,   889, 22871,   933,  4921, 11045,   441, 17310,\n","        17251,    61,   676,    13,     3,     9, 18024,     3,     9, 20230,\n","         4300,  3876,   682, 22871, 23740,   917,   533,    11,  5576,    11,\n","         5014,  2505,    13,   917, 22871, 26550, 11045,    38,   712,   500,\n","           13,   574,    28, 20230,  4300,  4818, 22871,   749,  3663,    15,\n","           11,   453,   750,   313,  8201,    28,     8, 20230,  4300,  4818,\n","           21,   997,   226,   940,  2453,    11,   380,   379, 15694,    11,\n","         3864,    87, 12304,    15,     7,     5, 11045,    56,    36,  2650,\n","           38,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor([   1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100])}\n"]}]},{"cell_type":"code","source":["for i in range(3):  # Checking 3 examples\n","    print(f\"Example {i+1}:\")\n","    print(\"Input IDs:\", tokenized_dataset[\"train\"][i][\"input_ids\"])\n","    print(\"Labels:\", tokenized_dataset[\"train\"][i][\"labels\"])\n","    print(\"=\"*80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5P4As5TXEHTB","executionInfo":{"status":"ok","timestamp":1742231165470,"user_tz":240,"elapsed":22,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}},"outputId":"c921d551-06c8-4e89-c4a4-8fa18dde0842"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Example 1:\n","Input IDs: tensor([   13,  3501,  1112,   831,    57,  7327, 22871,   282,     7,   343,\n","        11899, 19674,     6,   276,  9164,     7,    11, 18892,   937,    57,\n","         7327,    12,  2024,  3501,   982,     6,    38,  1316,     6,   441,\n","            8,  4732,    13,     8,  2786,  5968,  4330, 25266,     7, 22871,\n","        11985, 20230,  4300,  4012,  1576, 15694,    21,    46,   641,  3029,\n","          988,    13,   284,  3501,   556,   338,     8, 11045,  1068,   483,\n","          610,  4293, 13308,  1109,   127,  1576,  5941,   121,    19,   136,\n","          483,    16,     8,  1576,   381,    12,     8,   269,    13,     8,\n","          500,    57,     8,   889,  4818,     3,    18,     3,    15,     5,\n","          122,     5,    10,     3,   226,     5,    63,    61, 22871,  7740,\n","          136,  1081, 13320,    21,   917,   889,  1081,    11,    66,  7185,\n","         3864, 22871, 11985,   917,   889,  5298,  1112,     6, 15694,    11,\n","        18892,     6,    11,   136,  3864,    12,  7185,     5,   933,  4921,\n","        11045,    13,   136,   831,  3501,  5298,  1112,   906,    38,     3,\n","            9,   741,    13,     8,  1112,     6, 15694,    11, 18892,     6,\n","           11,    87,   127,  3864, 22871, 11985,   358,   889,  5298,  1112,\n","            6, 15694,    11, 18892,     6,    11,  3864,     5, 22871, 21582,\n","          136,   831,  3501,  5298,  1112,     3,   390,    30,  7327,    31,\n","            7,  5719, 22871,  7740,    66,   414,  1139,   380,   379,  3602,\n","           13,   917,  1139,    27,    26,     7,    11,  1637,    11,   414,\n","         1139,  4735, 12262,     7, 22871,  7740,  1502,    21, 31858,     6,\n","        11981,    11,    87,   127,     3, 15917,    53,    13,   136,   917,\n","           11,  3501,  4303,  2073, 22871, 21582,   136,   831, 31858,     6,\n","        11981,    11,    87,   127,     3, 15917,    53,    13,   136,   917,\n","           11,  3501,  4303,  2073, 22871,  7740,  7192,    21,   821,  1341,\n","        15577,    87,  1018, 26703,  3803, 22871, 21582,  2332,  3501,   821,\n","        23668,    11,   370,  1151,   821, 23668,     3,   390,    30,  1693,\n","           13,   821, 15905,   937,    57,  7327, 22871,   282,     7,   343,\n","         7740,  7192,    30,   821, 15905,    11,   821,   772,    24,     8,\n","          917,    11,  3501,   225,    36,  8413,   581,     6,   379,   136,\n","         1316,  1339,    11,    87,   127,  4943,     7,    12,  2868,   821,\n","          331, 22871, 21582, 10069,   821,  4891,    11,   331,  1232, 22871,\n","          282,     7,   343, 15192, 20230,  4300,  3379,    21,  5576,    11,\n","           87,   127,    70,  3053,    16,  2594,    41,   413,    87,  3035,\n","         2637,    61, 22871,   282,     7,   343,  7327,    16,  2505,  2231,\n","           11,  4891,    13,  7327,    18, 29189,    26,   821,  1339,    11,\n","        15905,    12,  2082,     3,    99,  1164,    19,  3085, 14007, 22871,\n","          282,     7,   343,  6767,    13,   821,    11,  2614,   331,    38,\n","          937,    57,  7327,    31,     7,   917,   599,     7,    61,   889,\n","          282,     7,   343, 22871, 21582,   821, 15220,     7,    12,     8,\n","          806, 20230,  4300,     3,   390,    30, 15552, 23668,  8755,    11,\n","         4293,  8794,    57,  7327,    11,   806,    12,  7327,   917,   599,\n","            7,    61, 22871,  6767,    13,   821,    11,  2614,   331,    38,\n","          937,    57,  1002,   889, 22871,   933,  4921, 11045,   441, 17310,\n","        17251,    61,   676,    13,     3,     9, 18024,     3,     9, 20230,\n","         4300,  3876,   682, 22871, 23740,   917,   533,    11,  5576,    11,\n","         5014,  2505,    13,   917, 22871, 26550, 11045,    38,   712,   500,\n","           13,   574,    28, 20230,  4300,  4818, 22871,   749,  3663,    15,\n","           11,   453,   750,   313,  8201,    28,     8, 20230,  4300,  4818,\n","           21,   997,   226,   940,  2453,    11,   380,   379, 15694,    11,\n","         3864,    87, 12304,    15,     7,     5, 11045,    56,    36,  2650,\n","           38,     1])\n","Labels: tensor([   1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100])\n","================================================================================\n","Example 2:\n","Input IDs: tensor([   71,  6828, 20186,  9215, 16662,   350, 16375,  4578,  3001,    71,\n","          377, 16375, 17226,   134,  2990,    71,     3, 27262,  3347, 30085,\n","         4083,   371, 17663,   434,  3001,   276,  5905, 20891,  4132,  1853,\n","         6157, 20788,   134,  3347,    71,   377, 16375, 17226,  4132,  9191,\n","         1853,   180, 17683,     3,  5946,  4211,  3430,  8472,   308,  3177,\n","        22164,  6157,    71, 10842,  5999,   377, 13162,   332,  7094, 10255,\n","            3, 19846,   476, 16309,  2365,  3430,     3, 17098,  3001,   276,\n","         5905, 20891,  4132,     3,  4611, 22177,  6157, 20788,   134,     6,\n","            3, 24833, 30050,   134, 12689,     3,  4138, 14594,   196, 20186,\n","         9215,  6828,  7094, 26417,    71,  6828, 20186,  9215, 16662,   350,\n","        16375,  4578,  1853,   377, 11253,  1881, 13506,    17,   454,  9169,\n","          377, 16375, 17226,   134,  2990,  1853,     3, 27262,  3001,  5686,\n","        21672,  4386,  1853,  6157, 20788,   134,  3347,    71,   377, 16375,\n","        17226,  4132,  5652,  1853,     3, 13845, 27481,  4674,     3, 13747,\n","         4763, 19056,   308,     3, 21712,  5078,  3347,   180, 20314,  6157,\n","        20788,   134,     3,  6058,  1853,   377, 16375, 17226,   134,  5080,\n","          454,  3291,     3,  6934, 29027,  2326,  1853,  5292,   518,   371,\n","         4254,  6828, 20186, 22164,  3347,  1853,   377, 16375, 17226,  4132,\n","         8859,  4386, 21221,  3430,   454,  3291,   377, 22862,  2326,  3001,\n","          205, 18290,  1853,     3,  6934, 29027,  3388,  1853,     3,  9312,\n","        18206, 31580,   308,  3388,     3,  4138, 14594,   196, 20186,  9215,\n","           27,     5,    41,   196,    61,    71,  6828, 20186,  9215,     3,\n","        15313, 22293,     3,  8742, 12604,   134,  1853,   377, 16375, 17226,\n","          134,  2990,  3001,     3, 25392,  5121,  4674,  3388, 25392,  5121,\n","         8472,  8575,   476,     6,    71,  4256,   196, 13738,     6,  4674,\n","            3, 21093,   518, 19056, 26585, 20805,     3, 21273,   411,  8775,\n","         8834,  8015,   134,  3001,     3, 19813, 18962, 10781,  8472, 11359,\n","         6227, 24596,   411,  8775,  8834,  8015,   134,  3001,  1853,   377,\n","        16375, 17226,   134,  5080,  4417, 12335,   134,  6828, 20186,  9215,\n","          454,  3291,   272, 23394,   283, 20458,  5652,  6828,  7765,   308,\n","         2365,  1853,  4083, 21672, 13729,  8472, 11359,  6227, 24596, 27407,\n","            5,  1853,   377,  9262, 16662,  1853,  4386,  6827,    71,  4486,\n","         8906,  3347, 12689,  3347, 20805,  2365,  9191,   377, 20129, 11951,\n","         1853,  8043, 16442,  4171,   476,   350, 25576, 21415, 30050,   134,\n","         4486,  8472, 23465,   382, 25002,     3,  2965, 17618, 21712,     6,\n","         4083,  6657,   329, 14920,  8015,     6,  4674,     3, 14920, 22500,\n","        21221,   272,   476,  1853,  8043, 16442,  4171,   476,   350, 25576,\n","        21415,     5,  1429,  1429,  1429,  1429,     3,  6058,  1853,   377,\n","        16375, 17226,   134,  2990,    31,   134,   283, 14464,  4083, 22851,\n","            3, 17630, 23190, 15397,  5097,  6048, 11810,   134,     3,  7451,\n","         4417, 22711,   196, 11430,  3430, 10046, 15251,    71,     3,  9978,\n","          549,  2990,  4611,  3347,   301, 10087,   332, 24634, 26954,  4200,\n","            6,  1853,   377, 16375, 17226,   134,  2990, 23716,     6,  8043,\n","         1853,  4083, 15367,  4209,  3347,  1853,   377, 16375, 17226,   134,\n","         5080,     6, 11155, 16375,  5042,  5652,  1853,     3, 25067, 26011,\n","         3347,  3388,  3177, 15397,    27, 17058,  6038, 11810,  3430,     3,\n","        21093, 26280,  3592,   276, 25167,   272,   476,  1853,   377, 16375,\n","        17226,   134,  5080,     3, 17161,  3502,  1853,   411,  8775,  8834,\n","         8015,   134,  3001, 31580, 17833,     3,  6038,  6048,     6,     3,\n","         5166, 17618,   553, 21221,   134,     6,   262, 21672,   345, 11810,\n","            6,  3388, 24992,  2990,   476,     6, 11466, 13570,  2365,     6,\n","         4674,     1])\n","Labels: tensor([   1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100])\n","================================================================================\n","Example 3:\n","Input IDs: tensor([   11, 19193,     5,   220,  9149,    10,  7833,  8257, 13227,  1853,\n","         4763,  5668,  6675, 21202,     6, 16003,     5,     6,   505,    18,\n","          439,     6,   204, 16936,    87, 22224,  1300,  2658,    96,  5890,\n","        26694,  1427, 21272,   179,     3, 29421,   127,    17,     7,   121,\n","          598,     6,    28,  1445,    12,     8,  2231,    12,    36,  8116,\n","        14550,    57,     3,     9,  3450,    28,  1445,    12,   136,  5997,\n","           41,    15,     5,   122,     5,     6,  2958, 23919,    11,  9747,\n","         1707,   270,  7248,   201,     8,   593,    13,  2231,  4700,    28,\n","            8,  2231,    11,  1438,   784, 10647,   908,    13,  1126,   512,\n","         1055,     6,    44,     3,     9,  1126,  1726,    16,   606,    42,\n","          556,   280, 10136,     6,   838,   139,   905,     8,  1726,    13,\n","          606,    42,   556,   280, 10136,    13,   119,    13,   784, 10647,\n","          908,   556,  4341,     6,  1455,    11, 13577,   447,  4710,     6,\n","          556,  3278,     6,   583,    13,  4968,     6,     8,  3265,   655,\n","           13,     8,  9895,     6,   224,   349,    31,     7,  7432,  1102,\n","           28,  1445,    12,   224,   556,    41,  5751,   224,   349,    31,\n","            7,  1418,    12,  3442,    42, 13321,     6,    42,    43,  5105,\n","           42, 13321,    26,     6,   224,  7432,  2166,   201,     8,  9879,\n","         3450,  7432,  3283,  2193,    12,     8,   556,     6,     8,  8253,\n","         1809,  1381,     6,     8, 17902,    13,  8253,  5142,     6,     8,\n","        17902,    11,  5996,    13, 12723,    42,  1805, 21624,    13,     8,\n","         5383,   556,     6,    11,   119,  2268,     6,  1281,     6,  4290,\n","           11,  1035,  4587,     7,     5,  6404,     3, 17979,     8,    21,\n","           15,  9545,     6,  9747,   120, 21272,   179,     3, 29421,   127,\n","           17,     7,  2311,     6,    28,  1445,    12,   224,  9958,     6,\n","           24,     3,     9,  3450,    10,    41,    23,    61, 16487, 12317,\n","         3263,    21,   224, 10472,    12,   806,  3490,   599,     7,    61,\n","          113,    33,  1213, 18704,    21,  2188,    11,  3393,   224,  2188,\n","           30,    46,    30,    18,  9545,  1873,     6,    41,    23,    23,\n","           61,   356,  7233,    21,  6771,    91,   224,  9958,     6,    11,\n","           41,    23,    23,    23,    61, 28974,  1438,   876,    12,  3245,\n","         2188,    28,  1445,    12,   224,  7233,     5,  1300,  2884,    96,\n","        25716,   121,    42,    96, 25716,  1361,    57,   121,   598,     6,\n","           28,  1445,    12,   136,  8900,    18,  7825,     6,    86, 13858,\n","            6, 20565,     6,   748,     6,  2405,  3535,     6, 10602,    42,\n","          119,  8445,   785,   269,     6,  8244,    57,     3,     9,  3450,\n","           42,   165, 24003,     7,    41, 26764,    57,  7915,     6,  3344,\n","         5334,    42,   119,   598,    61,    13,     8,  1281,   269,    12,\n","         5334,     8,   269,    12,   592,    42,   169,     6,    42,    12,\n","         5334,     3,     9,  3344,    42,     3,     9,   769, 28062,    12,\n","            6,   224,  8900,    18,  7825,     6,    86, 13858,     6, 20565,\n","            6,   748,     6,  2405,  3535,     6, 10602,    42,   119,  8445,\n","          785,   269,    38,   937,    21,   270,    77,   406, 27857,    53,\n","            8, 16950,  2166,    13,   136,  9879,  3450,    42,   136,  1353,\n","           13,   136,  2791,    42,   119,  8641,   344,   224,  3450,    41,\n","          127,   136,    13,   165, 24003,     7,    61,    11,   136,  9879,\n","         3450,     5,  1300,  2773,    96,   254,  1890,   121,    42,    96,\n","          254,  1890,    53,   121,   598,     6,    28,  1445,    12,     3,\n","            9,  1090,  1426,  1052,    44,   962,    11,     3,     9,  2193,\n","        20565,     6,    24,     8,  9421,     6,   169,     6,  1048,     6,\n","          462,    21,  1048,    42,  4830,   257,    13,   224,  1426,  1052,\n","          133,     1])\n","Labels: tensor([   1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100])\n","================================================================================\n"]}]},{"cell_type":"markdown","source":["Step 6: Fine-Tune FLAN-T5 with Hugging Face Trainer"],"metadata":{"id":"QQT7B27dSSoG"}},{"cell_type":"markdown","source":["checking for missing labels, and seeing model train parameters"],"metadata":{"id":"p9SpfxK5Cwg4"}},{"cell_type":"code","source":["# Check for missing labels\n","print(any(len(sample[\"labels\"]) == 0 for sample in tokenized_dataset[\"train\"]))\n","\n","# Check if at least some labels are not -100\n","print(any(any(label != -100 for label in sample[\"labels\"]) for sample in tokenized_dataset[\"train\"]))\n","\n","# Check model's trainable parameters\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"Trainable parameters: {trainable_params}/{total_params}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kuRnNagOAHYk","executionInfo":{"status":"ok","timestamp":1742231162321,"user_tz":240,"elapsed":139186,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}},"outputId":"727f14c9-4e14-4dc8-8409-5d7143f9ecd1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n","True\n","Trainable parameters: 28317696/247577856\n"]}]},{"cell_type":"markdown","source":["- False for missing labels: This means that all samples in the dataset have labels, so there are no empty label sequences. So, the dataset is structured correctly.\n","- True for at least some labels being non--100: This means that at least some training samples contain valid, learnable tokens (not all are -100). This is a necessary condition for training to work.\n","- Trainable parameters: 28,317,696/247,577,856: This shows that you froze most of the model and are fine-tuning only ~28M parameters out of ~247M total parameters. If you intentionally froze the lower layers (as planned), this is expected."],"metadata":{"id":"KCbsYnD0DIyz"}},{"cell_type":"code","source":["from transformers import Trainer\n","\n","# Define the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_dataset[\"validation\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pdwkRnAZSTV9","executionInfo":{"status":"ok","timestamp":1742233475605,"user_tz":240,"elapsed":13,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}},"outputId":"7e4826b8-89a1-46b2-a6a4-7536056afa88"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-24-2579a87eed51>:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]}]},{"cell_type":"code","source":["import torch\n","\n","# Get a single sample from the training dataset\n","sample = tokenized_dataset[\"train\"][0]\n","\n","# Convert to PyTorch tensors and move to device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0).to(device)  # Add batch dimension\n","attention_mask = torch.tensor(sample[\"attention_mask\"]).unsqueeze(0).to(device)\n","labels = torch.tensor(sample[\"labels\"]).unsqueeze(0).to(device)\n","\n","# Verify the tensors exist\n","print(\"input_ids shape:\", input_ids.shape)\n","print(\"attention_mask shape:\", attention_mask.shape)\n","print(\"labels shape:\", labels.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UVhytqxsJj0S","executionInfo":{"status":"ok","timestamp":1742232507981,"user_tz":240,"elapsed":45,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}},"outputId":"1e1b3752-bfb3-485f-a81f-2ab10a0ce2d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["input_ids shape: torch.Size([1, 512])\n","attention_mask shape: torch.Size([1, 512])\n","labels shape: torch.Size([1, 128])\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-18-fe047e14ac10>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0).to(device)  # Add batch dimension\n","<ipython-input-18-fe047e14ac10>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  attention_mask = torch.tensor(sample[\"attention_mask\"]).unsqueeze(0).to(device)\n","<ipython-input-18-fe047e14ac10>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels = torch.tensor(sample[\"labels\"]).unsqueeze(0).to(device)\n"]}]},{"cell_type":"code","source":["with torch.no_grad():\n","    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","    print(\"Loss:\", outputs.loss.item())  # Must be a finite number, NOT nan\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GUbixOycHrt3","executionInfo":{"status":"ok","timestamp":1742232513512,"user_tz":240,"elapsed":1000,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}},"outputId":"fc1eb5ff-36aa-423a-c44e-7a5d50cc2077"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"]},{"output_type":"stream","name":"stdout","text":["Loss: 8.835987091064453\n"]}]},{"cell_type":"code","source":["# Start training\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":394},"id":"AQiy4ntpK-aD","executionInfo":{"status":"error","timestamp":1742237674671,"user_tz":240,"elapsed":4183693,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}},"outputId":"73866b6c-491b-4472-9c0a-0560d334b091"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='13653' max='21942' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [13653/21942 1:09:43 < 42:19, 3.26 it/s, Epoch 1.24/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.000000</td>\n","      <td>nan</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-635967aca760>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2171\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2172\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2173\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2529\u001b[0m                     )\n\u001b[1;32m   2530\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2531\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2533\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3674\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3675\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3677\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3729\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3730\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3731\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3732\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3733\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1892\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1122\u001b[0m                 )\n\u001b[1;32m   1123\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1125\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mcache_position\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     ):\n\u001b[0;32m--> 675\u001b[0;31m         self_attention_outputs = self.layer[0](\n\u001b[0m\u001b[1;32m    676\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    591\u001b[0m     ):\n\u001b[1;32m    592\u001b[0m         \u001b[0mnormed_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         attention_output = self.SelfAttention(\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mnormed_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["Step 7: Save Fine-Tuned Model"],"metadata":{"id":"tBDWjc4lYk0s"}},{"cell_type":"code","source":["# Save the fine-tuned model and tokenizer\n","model.save_pretrained(\"/content/drive/My Drive/Colab Notebooks/NLP_266_Project/model_checkpoints/flan_t5_finetuned-1\")\n","tokenizer.save_pretrained(\"/content/drive/My Drive/Colab Notebooks/NLP_266_Project/model_checkpoints/flan_t5_finetuned-1\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"409IF9VkYlvk","executionInfo":{"status":"ok","timestamp":1742196047114,"user_tz":240,"elapsed":2812,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}},"outputId":"a6a26f09-6b2f-4491-8b45-1f53e5a6a50a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('/content/drive/My Drive/Colab Notebooks/NLP_266_Project/model_checkpoints/flan_t5_finetuned-1/tokenizer_config.json',\n"," '/content/drive/My Drive/Colab Notebooks/NLP_266_Project/model_checkpoints/flan_t5_finetuned-1/special_tokens_map.json',\n"," '/content/drive/My Drive/Colab Notebooks/NLP_266_Project/model_checkpoints/flan_t5_finetuned-1/spiece.model',\n"," '/content/drive/My Drive/Colab Notebooks/NLP_266_Project/model_checkpoints/flan_t5_finetuned-1/added_tokens.json',\n"," '/content/drive/My Drive/Colab Notebooks/NLP_266_Project/model_checkpoints/flan_t5_finetuned-1/tokenizer.json')"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["Run a quick loss check using a few samples before training:\n","If this prints 0.0, the model is failing to compute loss.\n","If the loss is non-zero, then the issue is in the training loop."],"metadata":{"id":"9OKCRHpqDg-Q"}},{"cell_type":"code","source":["import torch\n","\n","# Take a small batch of training data\n","batch = tokenized_dataset[\"train\"][:8]\n","\n","# Move to GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# Convert to tensors\n","input_ids = torch.tensor(batch[\"input_ids\"]).to(device)\n","attention_mask = torch.tensor(batch[\"attention_mask\"]).to(device)\n","labels = torch.tensor(batch[\"labels\"]).to(device)\n","\n","# Compute loss\n","with torch.no_grad():\n","    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","    print(\"Loss:\", outputs.loss.item())  # Should not be 0.0\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MT-fQpQVC5Wa","executionInfo":{"status":"ok","timestamp":1742197205446,"user_tz":240,"elapsed":75,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}},"outputId":"39f94b41-fcb2-4208-94b2-297c61f1c96d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loss: nan\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-27-09e5b39d8137>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_ids = torch.tensor(batch[\"input_ids\"]).to(device)\n","<ipython-input-27-09e5b39d8137>:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  attention_mask = torch.tensor(batch[\"attention_mask\"]).to(device)\n","<ipython-input-27-09e5b39d8137>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels = torch.tensor(batch[\"labels\"]).to(device)\n"]}]},{"cell_type":"markdown","source":["Step 8: Generate Predictions and Evaluate Performance\n","After fine-tuning, we generate predictions and evaluate the model."],"metadata":{"id":"H-SxSzJtawvt"}},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","def generate_predictions(model, tokenizer, dataset, batch_size=16):\n","    \"\"\"\n","    Generate predictions using the fine-tuned FLAN-T5 model.\n","    \"\"\"\n","    model.eval()\n","    model.to(device)\n","\n","    predictions, references, doc_ids_list = [], [], []\n","\n","    for batch in tqdm(dataset[\"test\"], desc=\"Generating Predictions\"):\n","        input_ids = batch[\"input_ids\"].unsqueeze(0).to(device)\n","        attention_mask = batch[\"attention_mask\"].unsqueeze(0).to(device)\n","\n","        with torch.no_grad():\n","            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=128)\n","\n","        preds = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","        # Check if 'labels' are within the acceptable range for the tokenizer\n","        refs = tokenizer.decode(batch[\"labels\"].clamp(min=0, max=tokenizer.vocab_size - 1), skip_special_tokens=True)\n","\n","        predictions.append(preds)\n","        references.append(refs)\n","        doc_ids_list.append(batch[\"doc_id\"])\n","\n","    return predictions, references, doc_ids_list\n","\n","# Run inference\n","chunk_predictions, chunk_references, chunk_doc_ids = generate_predictions(model, tokenizer, tokenized_dataset, batch_size=16)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339},"id":"2OxsQfewaxhr","executionInfo":{"status":"error","timestamp":1742196197281,"user_tz":240,"elapsed":55383,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}},"outputId":"d263e8d2-5ccc-4907-82fd-6264acf41119"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Generating Predictions:   0%|          | 39/43886 [00:55<17:16:21,  1.42s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-d178c1b8e4b9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Run inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mchunk_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_references\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_doc_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-20-d178c1b8e4b9>\u001b[0m in \u001b[0;36mgenerate_predictions\u001b[0;34m(model, tokenizer, dataset, batch_size)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2255\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2256\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2257\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3255\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3256\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3257\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3259\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1892\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1122\u001b[0m                 )\n\u001b[1;32m   1123\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1125\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mdo_cross_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_decoder\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_cross_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             cross_attention_outputs = self.layer[1](\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 \u001b[0mkey_value_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0mcache_position\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m     ):\n\u001b[0;32m--> 628\u001b[0;31m         \u001b[0mnormed_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m         attention_output = self.EncDecAttention(\n\u001b[1;32m    630\u001b[0m             \u001b[0mnormed_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariance\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariance_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;31m# convert into half-precision if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","from tqdm import tqdm\n","\n","def generate_predictions(model, tokenizer, dataset, batch_size=32):\n","    \"\"\"\n","    Generate text predictions for a dataset using a trained transformer model,\n","    while tracking progress with a progress bar.\n","    \"\"\"\n","\n","    # Set model to evaluation mode (disables dropout, batch norm, etc.)\n","    model.eval()\n","\n","    # Ensure the model is on the correct device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)  # Move the entire model to the device\n","\n","    # Convert dataset to PyTorch tensors and move to the same device\n","    # Ensure all tensors are on the same device before creating the TensorDataset\n","    input_ids = dataset[\"input_ids\"].clone().detach().to(device)\n","    attention_mask = dataset[\"attention_mask\"].clone().detach().to(device)\n","    labels = dataset[\"labels\"].clone().detach().to(device)\n","    doc_ids = dataset[\"doc_id\"].clone().detach().to(device)\n","\n","    # Create a TensorDataset with all tensors on the same device\n","    tensor_dataset = TensorDataset(input_ids, attention_mask, labels, doc_ids)\n","\n","    # Use DataLoader with fewer workers to prevent GPU errors\n","    dataloader = DataLoader(\n","        tensor_dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=0  # Keep num_workers=0 to avoid multiprocessing issues with CUDA\n","    )\n","\n","    # Use tqdm to track progress\n","    progress_bar = tqdm(dataloader, total=len(dataloader), desc=\"Generating Predictions\", unit=\"batch\")\n","\n","    predictions, references, doc_ids_list = [], [], []\n","\n","    for input_ids, attention_mask, labels, doc_ids_batch in progress_bar:\n","        # No need to move tensors to device again here as they are already on the device\n","\n","        with torch.no_grad():  # Disable gradients for faster inference\n","            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=128)\n","\n","        # Decode the generated tokenized outputs into human-readable text\n","        preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","\n","        # Process reference labels (replace -100 with pad token for decoding)\n","        labels_for_decoding = [\n","            [label_id if label_id != -100 else tokenizer.pad_token_id for label_id in label]\n","            for label in labels\n","        ]\n","        refs = tokenizer.batch_decode(labels_for_decoding, skip_special_tokens=True)\n","\n","        # Store results\n","        predictions.extend(preds)\n","        references.extend(refs)\n","        doc_ids_list.extend(doc_ids_batch.tolist())  # Convert to Python list\n","\n","        # Update progress bar with completed count\n","        progress_bar.set_postfix({\"Completed\": f\"{len(predictions)}/{len(dataset)}\"})\n","\n","    progress_bar.close()\n","\n","    return predictions, references, doc_ids_list\n","\n","chunk_predictions, chunk_references, chunk_doc_ids = generate_predictions(model, tokenizer, tokenized_dataset[\"test\"], batch_size=16)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339},"id":"vF-vBECF_XJv","executionInfo":{"status":"error","timestamp":1742196386707,"user_tz":240,"elapsed":19265,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}},"outputId":"f961f3cd-b6fb-4d09-ee02-5e3deee00030"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Generating Predictions:   0%|          | 4/2743 [00:18<3:34:13,  4.69s/batch, Completed=64/43886]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-f641840c9173>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_ids_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mchunk_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_references\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_doc_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-23-f641840c9173>\u001b[0m in \u001b[0;36mgenerate_predictions\u001b[0;34m(model, tokenizer, dataset, batch_size)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Decode the generated tokenized outputs into human-readable text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Process reference labels (replace -100 with pad token for decoding)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_decode\u001b[0;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3809\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdecoded\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3810\u001b[0m         \"\"\"\n\u001b[0;32m-> 3811\u001b[0;31m         return [\n\u001b[0m\u001b[1;32m   3812\u001b[0m             self.decode(\n\u001b[1;32m   3813\u001b[0m                 \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3810\u001b[0m         \"\"\"\n\u001b[1;32m   3811\u001b[0m         return [\n\u001b[0;32m-> 3812\u001b[0;31m             self.decode(\n\u001b[0m\u001b[1;32m   3813\u001b[0m                 \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m                 \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3849\u001b[0m         \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_py_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3851\u001b[0;31m         return self._decode(\n\u001b[0m\u001b[1;32m   3852\u001b[0m             \u001b[0mtoken_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3853\u001b[0m             \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m         )\n\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m     def _decode(\n\u001b[0m\u001b[1;32m    658\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0mtoken_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import gc\n","gc.collect()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aKERhIop_g_U","executionInfo":{"status":"ok","timestamp":1742196317521,"user_tz":240,"elapsed":6,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}},"outputId":"f1add56e-87cc-4aca-dd7c-7a1ad843cdf5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["542"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["### Experiment 2: Fine tuning with additional prompt in inputs.\n","we can further enhance FLAN-T5 fine-tuning by incorporating additional instruction-based prompts. Since FLAN-T5 is specifically trained on instruction-following tasks, we can augment the dataset by explicitly instructing the model on how to extract clauses. This will help guide the model toward more accurate predictions, especially in cases where the baseline struggled with hallucinations and incomplete responses.\n","\n","We will need to preprocess the original dataset to:\n","- Instead of just using the original extraction questions, we can improve the model's understanding of the task by adding explicit instructions that tells it what to look for: `\"You are an AI trained for legal clause extraction. Carefully read the contract and extract only the exact document name. If no document name is present, return 'No document name found'. Extract the Document Name from this document: <contract text>\"`\n","    - This additional instruction clarifies:\n","    - The model's role\n","    - The expected response format\n","    - What to do when the clause does not exist. This aligns better with the original CUAD dataset, which includes many \"impossible\" answers.\n","    - Instead of outputting random hallucinated text, the model can explicitly - state that a clause is not present.\n","    - Contracts may have long or repeated sections, causing chunk confusion.\n","    - Explicit prompts help reinforce the clause type being extracted.\n","\n","\n","To accomplish this, we will need to;\n","1. Preprocess the original dataset again to add desired prompt.\n","2. Explore to either add contextual chunking or Hierarchical retrieval where we retrieve the most relevant sections before feeding them to the model. We can also pair this with contextual chunking as opposed to our previous approach of fixed chunking at 128. If, we are chunking, we need to match labels to relevant chunks.\n","3. Convert the final dataset into a hugging face dataset and split 80-10-10\n","4. Tokenize the dataset and start training.\n","5. Explore what layers of model to freeze or unfreeze based on our results from flan5 model 1.\n","6. Train the model\n","7. Inference"],"metadata":{"id":"f5oGE7Z5WLWq"}},{"cell_type":"markdown","source":["#### Step 1: Re-processing the original CUAD dataset and extracting question-answer pairs then adding additional prompt with explicit instructions on how the model should handle tasks."],"metadata":{"id":"mSNfRp29dlJn"}},{"cell_type":"code","source":["import json\n","import re\n","\n","# Load the original CUAD dataset\n","with open('/content/drive/My Drive/Colab Notebooks/CUAD_v1.json', 'r', encoding='utf-8') as file:\n","    dataset = json.load(file)[\"data\"]\n","\n","# Function to clean text\n","def clean_text(text):\n","    text = text.strip()\n","    text = re.sub(r'\\s+', ' ', text)\n","    return text\n","\n","# Preprocess dataset with more instructional prompts\n","processed_data = []\n","\n","for contract in dataset:\n","    contract_title = clean_text(contract[\"title\"])\n","\n","    for paragraph in contract[\"paragraphs\"]:\n","        context = clean_text(paragraph[\"context\"])\n","\n","        for qa in paragraph[\"qas\"]:\n","            clause_type = clean_text(qa[\"question\"].split(\"related to \\\"\")[1].split(\"\\\"\")[0])\n","            answers = [clean_text(ans[\"text\"]) for ans in qa[\"answers\"]] if not qa[\"is_impossible\"] else [\"\"]\n","\n","            # **New Instruction-Based Prompt**\n","            instruction_prompt = (\n","                f\"You are an AI trained for legal clause extraction. \"\n","                f\"Carefully read the contract and extract only the exact {clause_type}. \"\n","                f\"If the {clause_type} is missing, return 'No {clause_type} found'.\\n\\n\"\n","                f\"Extract the {clause_type} from this document: {context}\"\n","            )\n","\n","            # Store in dataset format\n","            processed_data.append({\n","                \"contract_title\": contract_title,\n","                \"input\": instruction_prompt,\n","                \"expected_output\": answers\n","            })\n","\n","# Save processed data for later fine-tuning\n","with open('/content/drive/My Drive/Colab Notebooks/NLP_266_Project/Data/instruction_finetuned_data.json', 'w') as file:\n","    json.dump(processed_data, file, indent=4)\n","\n","print(f\"Processed {len(processed_data)} examples with instructional prompts.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YwPig67STL2h","executionInfo":{"status":"ok","timestamp":1742184792127,"user_tz":240,"elapsed":11341,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}},"outputId":"180b3683-df3f-4116-f162-d320d2ace41b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processed 20910 examples with instructional prompts.\n"]}]},{"cell_type":"code","source":["processed_data[90]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OfhaUUcsTnjG","executionInfo":{"status":"ok","timestamp":1742185340256,"user_tz":240,"elapsed":6,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}},"outputId":"3fad8371-b281-49e9-a0fb-06bb591b3b2c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'contract_title': 'LohaCompanyltd_20191209_F-1_EX-10.16_11917878_EX-10.16_Supply Agreement',\n"," 'input': 'You are an AI trained for legal clause extraction. Carefully read the contract and extract only the exact Most Favored Nation. If the Most Favored Nation is missing, return \\'No Most Favored Nation found\\'.\\n\\nExtract the Most Favored Nation from this document: Exhibit 10.16 SUPPLY CONTRACT Contract No: Date: The buyer/End-User: Shenzhen LOHAS Supply Chain Management Co., Ltd. ADD: Tel No. : Fax No. : The seller: ADD: The Contract is concluded and signed by the Buyer and Seller on , in Hong Kong. 1. General provisions 1.1 This is a framework agreement, the terms and conditions are applied to all purchase orders which signed by this agreement (hereinafter referred to as the \"order\"). 1.2 If the provisions of the agreement are inconsistent with the order, the order shall prevail. Not stated in order content will be subject to the provisions of agreement. Any modification, supplementary, give up should been written records, only to be valid by buyers and sellers authorized representative signature and confirmation, otherwise will be deemed invalid. 2. The agreement and order 2.1 During the validity term of this agreement, The buyer entrust SHENZHEN YICHANGTAI IMPORT AND EXPORT TRADE CO., LTD or SHENZHEN LEHEYUAN TRADING CO, LTD (hereinafter referred to as the \"entrusted party\" or \"YICHANGTAI\" or \"LEHEYUAN\"), to purchase the products specified in this agreement from the seller in the form of orders. 2.2 The seller shall be confirmed within three working days after receipt of order. If the seller finds order is not acceptable or need to modify, should note entrusted party in two working days after receipt of the order, If the seller did not confirm orders in time or notice not accept orders or modifications, the seller is deemed to have been accepted the order. The orders become effective once the seller accepts, any party shall not unilaterally cancel the order before the two sides agreed . 2.3 If the seller puts forward amendments or not accept orders, the seller shall be in the form of a written notice to entrusted party, entrusted party accept the modified by written consent, the modified orders to be taken effect. 2.4 Seller\\'s note, only the buyer entrust the entrusted party issued orders, the product delivery and payment has the force of law. 1 Source: LOHA CO. LTD., F-1, 12/9/2019 3. GOODS AND COUNTRY OF ORIGIN: 4. Specific order: The products quantity, unit price, specifications, delivery time and transportation, specific content shall be subject to the purchase order issued by entrusted party which is commissioned the buyer. 5. PACKING: To be packed in new strong wooden case(s) /carton(s), suitable for long distance transportation and for the change of climate, well protected against rough handling, moisture, rain, corrosion, shocks, rust, and freezing. The seller shall be liable for any damage and loss of the commodity, expenses incurred on account of improper packing, and any damage attributable to inadequate or improper protective measures taken by the seller in regard to the packing. One full set of technical All wooden material of shipping package must be treated as the requirements of Entry-Exit Inspection and Quarantine Bureau of China, by the agent whom is certified by the government where the goods is exported. And the goods must be marked with the IPPC stamps, which are certified by the government agent of Botanical-Inspection and Quarantine Bureau. 6. SHIPPING MARK: The Sellers shall mark on each package with fadeless paint the package number, gross weight, net weight, measurements and the wordings: \"KEEP AWAY FROM MOISTURE\",\"HANDLE WITH CARE\" \"THIS SIDE UP\" etc. and the shipping mark on each package with fadeless paint. 7. DATE OF SHIPMENT: According to specific order by YICHANGTAI or LEHEYUAN. 8. PORT OF SHIPMENT: 2 Source: LOHA CO. LTD., F-1, 12/9/2019 9. PORT OF DESTINATION: SHENZHEN, GUANGDONG, CHINA 10. INSURANCE: To be covered by the Seller for 110% invoice value against All Risks and War Risk. 11. PAYMENT: Under Letter of Credit or T/T: Under the Letter of Credit: The Buyer shall open an irrevocable letter of credit with the bank within 30 days after signing the contract, in favor of the Seller, for 100% value of the total contract value. The letter of credit should state that partial shipments are allowed. The Buyer\\'s agent agrees to pay for the goods in accordance with the actual amount of the goods shipped. 80% of the system value being shipped will be paid against the documents stipulated in Clause 12.1. The remaining 20% of the system value being shipped will be paid against the documents stipulated in Clause 12.2. The Letter of Credit shall be valid until 90 days after the latest shipment is effected. Under the T/T The trustee of the buyer remitted the goods to the seller by telegraphic transfer in batches as agreed upon after signing each order. 12. DOCUMENTS: 12.1 (1) Invoice in 5 originals indicating contract number and Shipping Mark (in case of more than one shipping mark, the invoice shall be issued separately). (2) One certificate of origin of the goods. (3) Four original copies of the packing list. (4) Certificate of Quality and Quantity in 1 original issued by the agriculture products base. (5) One copy of insurance coverage (6) Copy of cable/letter to the transportation department of Buyer advising of particulars as to shipment immediately after shipment is made. 3 Source: LOHA CO. LTD., F-1, 12/9/2019 12.2 (1) Invoice in 3 originals indicating contract number and L/C number. (2) Final acceptance certificate signed by the Buyer and the Seller. 13. SHIPMENT: CIP The seller shall contract on usual terms at his own expenses for the carriage of the goods to the agreed point at the named place of destination and bear all risks and expenses until the goods have been delivered to the port of destination. The Sellers shall ship the goods within the shipment time from the port of shipment to the port of destination. Transshipment is allowed. Partial Shipment is allowed. In case the goods are to be dispatched by parcel post/sea-freight, the Sellers shall, 3 days before the time of delivery, inform the Buyers by cable/letter of the estimated date of delivery, Contract No., commodity, invoiced value, etc. The sellers shall, immediately after dispatch of the goods, advise the Buyers by cable/letter of the Contract No., commodity, invoiced value and date of dispatch for the Buyers. 14. SHIPPING ADVICE: The seller shall within 72 hours after the shipment of the goods, advise the shipping department of buyer by fax or E-mail of Contract No., goods name, quantity, value, number of packages, gross weight, measurements and the estimated arrival time of the goods at the destination. 15. GUARANTEE OF QUALITY: The Sellers guarantee that the commodity hereof is complies in all respects with the quality and specification stipulated in this Contract. 16. CLAIMS: Within 7 days after the arrival of the goods at destination, should the quality, specification, or quantity be found not in conformity with the stipulations of the Contract except those claims for which the insurance company or the owners of the vessel are liable, the Buyers, on the strength of the Inspection Certificate issued by the China Commodity Inspection Bureau, have the right to claim for replacement with new goods, or for compensation, and all the expenses (such as inspection charges, freight for returning the goods and for sending the replacement, insurance premium, storage and loading and unloading charges etc.) shall be borne by the Sellers. The Certificate so issued shall be accepted as the base of a claim. The Sellers, in accordance with the Buyers\\' claim, shall be responsible for the immediate elimination of the defect(s), complete or partial replacement of the commodity or shall devaluate the commodity according to the state of defect(s). Where necessary, the Buyers shall be at liberty to eliminate the defect(s) themselves at the Sellers\\' expenses. If the Sellers fail to answer the Buyers within one weeks after receipt of the aforesaid claim, the claim shall be reckoned as having been accepted by the Sellers. 4 Source: LOHA CO. LTD., F-1, 12/9/2019 17. FORCE MAJEURE: The Sellers shall not be held responsible for the delay in shipment or non-delivery, of the goods due to Force Majeure, which might occur during the process of manufacturing or in the course of loading or transit. The Sellers shall advise the Buyers immediately of the occurrence mentioned above and within fourteen days thereafter, the Sellers shall send by airmail to the Buyers a certificate of the accident issued by the competent government authorities, Chamber of Commerce or registered notary public of the place where the accident occurs as evidence thereof. Under such circumstances the Sellers, however, are still under the obligation to take all necessary measures to hasten the delivery of the goods. In case the accident lasts for more than 10 weeks, the Buyers shall have the right to cancel the Contract. 18. LATE DELIVERY AND PENALTY: Should the Sellers fail to make delivery on time as stipulated in the Contract, with exception of Force Majeure causes specified in Clause 17 of this Contract, the Buyers shall agree to postpone the delivery on condition that the Sellers agree to pay a penalty which shall be deducted by the paying bank from the payment. The penalty, however, shall not exceed 5% of the total value of the goods involved in the late delivery. The rate of penalty is charged at 0.5% for every seven days, odd days less than seven days should be counted as seven days. In case the Sellers fail to make delivery ten weeks later than the time of shipment stipulated in the Contract, the Buyers have the right to cancel the contract and the Sellers, in spite of the cancellation, shall still pay the aforesaid penalty to the Buyers without delay, the seller should refund the money received and pay the 30% of the total goods price of the penalty 19. ARBITRATION: All disputes in connection with this Contract or the execution thereof shall be settled friendly through negotiations. In case no settlement can be reached, the case may then be submitted for arbitration to the Foreign Economic and Trade Arbitration Committee of the China Beijing Council for the Promotion of International Trade in accordance with its Provisional Rules of Procedures by the said Arbitration Committee. The Arbitration shall take place in Beijing and the decision of the Arbitration Committee shall be final and binding upon both parties; neither party shall seek recourse to a law court nor other authorities to appeal for revision of the decision. Arbitration fee shall be borne by the losing party. 20. This final price is the confidential information. Dissemination, distribution or duplication of this price is strictly prohibited. 5 Source: LOHA CO. LTD., F-1, 12/9/2019 21. Law application It will be governed by the law of the People\\'s Republic of China ,otherwise it is governed by United Nations Convention on Contract for the International Sale of Goods. 22. <<Incoterms 2000>> The terms in the contract are based on (INCOTERMS 2000) of the International Chamber of Commerce. 23. The Contract is valid for 5 years, beginning from and ended on . This Contract is made out in three originals in both Chinese and English, each language being legally of the equal effect. Conflicts between these two languages arising there from, if any, shall be subject to Chinese version. One copy for the Sellers, two copies for the Buyers. The Contract becomes effective after signed by both parties. THE BUYER: THE SELLER: SIGNATURE: SIGNATURE: 6 Source: LOHA CO. LTD., F-1, 12/9/2019',\n"," 'expected_output': ['']}"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["#### Step 2: Convert the Modified Dataset into a Hugging Face Dataset Format\n","Now that the dataset has explicit instruction prompts, we must convert it into a Hugging Face dataset format for training."],"metadata":{"id":"9TqYBndHfNNx"}},{"cell_type":"code","source":["import datasets\n","from datasets import Dataset, DatasetDict\n","\n","# Convert to Hugging Face Dataset format\n","hf_dataset = Dataset.from_list(chunked_dataset)\n","\n","# Split into train (80%), validation (10%), and test (10%)\n","train_test_split = hf_dataset.train_test_split(test_size=0.2, seed=42)\n","valid_test_split = train_test_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n","\n","# Correctly label the dataset splits\n","dataset = DatasetDict({\n","    \"train\": train_test_split[\"train\"],\n","    \"validation\": valid_test_split[\"train\"],  # Corrected label\n","    \"test\": valid_test_split[\"test\"]  # Corrected label\n","})\n"],"metadata":{"id":"QTzLwHgpenwg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Step 3: Fine-Tune FLAN-T5 with the New Instruction-Based Dataset\n","Now, load FLAN-T5 and fine-tune it using this improved dataset."],"metadata":{"id":"MfTKbnsWfVgG"}},{"cell_type":"code","source":["from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer\n","from datasets import load_from_disk\n","\n","# Load tokenizer and model\n","model_name = \"google/flan-t5-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","\n","# Load the updated dataset\n","dataset_path = \"/content/drive/My Drive/Colab Notebooks/NLP_266_Project/instruction_finetuned_dataset\"\n","tokenized_dataset = load_from_disk(dataset_path)\n","\n","# Freeze lower layers\n","for param in model.parameters():\n","    param.requires_grad = False  # Freeze all layers\n","\n","num_layers_to_unfreeze = 4\n","for layer in model.encoder.block[-num_layers_to_unfreeze:]:  # Unfreeze last N layers\n","    for param in layer.parameters():\n","        param.requires_grad = True\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./flan_t5_instruction_finetuned\",\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    logging_strategy=\"steps\",\n","    learning_rate=3e-5,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    weight_decay=0.01,\n","    save_total_limit=2,\n","    num_train_epochs=3,\n","    fp16=True,\n","    logging_dir=\"./logs\",\n","    logging_steps=500,\n","    report_to=\"none\"\n",")\n","\n","# Define trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_dataset[\"validation\"],\n","    tokenizer=tokenizer\n",")\n","\n","# Train model\n","trainer.train()\n","\n","# Save the fine-tuned model\n","model.save_pretrained(\"/content/drive/My Drive/Colab Notebooks/NLP_266_Project/flan_t5_instruction_finetuned\")\n","tokenizer.save_pretrained(\"/content/drive/My Drive/Colab Notebooks/NLP_266_Project/flan_t5_instruction_finetuned\")\n"],"metadata":{"id":"uvPUAZ4CfWZK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Step 4: Evaluate the Instruction-Tuned FLAN-T5\n","Once the fine-tuning is complete, we generate predictions and evaluate performance."],"metadata":{"id":"dmllb0HZfkIW"}},{"cell_type":"code","source":["from tqdm import tqdm\n","import evaluate\n","import numpy as np\n","\n","# Load evaluation metrics\n","rouge = evaluate.load(\"rouge\")\n","bleu = evaluate.load(\"bleu\")\n","\n","def generate_predictions(model, tokenizer, dataset):\n","    model.eval()\n","    model.to(\"cuda\")\n","\n","    predictions, references = [], []\n","\n","    for sample in tqdm(dataset[\"test\"], desc=\"Generating Predictions\"):\n","        input_ids = sample[\"input_ids\"].unsqueeze(0).to(\"cuda\")\n","        attention_mask = sample[\"attention_mask\"].unsqueeze(0).to(\"cuda\")\n","\n","        with torch.no_grad():\n","            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=128)\n","\n","        preds = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        refs = tokenizer.decode(sample[\"labels\"], skip_special_tokens=True)\n","\n","        predictions.append(preds)\n","        references.append(refs)\n","\n","    return predictions, references\n","\n","# Run inference\n","chunk_predictions, chunk_references = generate_predictions(model, tokenizer, tokenized_dataset)\n","\n","# Compute evaluation metrics\n","fine_tuned_metrics = compute_metrics(chunk_predictions, chunk_references)\n","print(\"Fine-Tuned Model Metrics with Instructions:\", fine_tuned_metrics)\n"],"metadata":{"id":"TkuXx6EiflC9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Expected Improvements\n","- Higher ROUGE and BLEU scores due to improved instruction clarity.\n","- Better handling of missing clauses with explicit \"No clause found\" responses.\n","- Reduced hallucinations, leading to fewer incorrect extractions.\n","- More structured outputs, as FLAN-T5 will understand legal text patterns better.\n","\n","This method leverages FLAN-T5’s instruction-following capabilities, significantly improving contract clause extraction."],"metadata":{"id":"hP34iZK3fwC6"}},{"cell_type":"code","source":["import json\n","import pandas as pd\n","import re\n","import torch\n","from transformers import AutoTokenizer\n","from tqdm import tqdm  # Import tqdm for progress tracking\n","\n","# 🔹 Load preprocessed contracts_data (our new dataset)\n","contracts_data = contracts_data  # This should be the cleaned dataset\n","\n","# 🔹 Load tokenizer and move to GPU if available\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model_name = \"google/flan-t5-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n","MAX_TOKENS = 512\n","OVERLAP = 256\n","\n","# 🔹 Step 1: Extract unique documents (Avoid redundant tokenization)\n","def extract_unique_documents(contracts_data):\n","    \"\"\"Creates a dictionary mapping each unique document ID to its full text.\"\"\"\n","    doc_map = {}\n","    # Wrap the loop with tqdm for progress tracking\n","    for contract in tqdm(contracts_data, desc=\"Extracting unique documents\"):\n","        doc_id = contract[\"doc_id\"]\n","        if doc_id not in doc_map:\n","            doc_map[doc_id] = contract[\"input\"].split(\"CONTEXT: \")[1].strip()  # Extract only document text\n","    return doc_map\n","\n","def chunk_documents(doc_map):\n","    \"\"\"Chunks each document efficiently using batch tokenization.\"\"\"\n","    chunked_docs = {}\n","\n","    # Wrap the outer loop with tqdm for progress tracking\n","    for doc_id, text in tqdm(doc_map.items(), desc=\"Chunking documents\"):\n","        text = text.replace(\"\\n\", \" \").strip()  # Remove unnecessary newlines & spaces\n","        sentences = text.split(\". \")  # Keep sentence boundaries\n","\n","        chunks = []\n","        current_chunk = []\n","        token_count = 0\n","        total_chunks = 0\n","\n","        # 🔹 Tokenize sentences in BATCHES (GPU-accelerated)\n","        sentence_tokens = tokenizer.batch_encode_plus(\n","            sentences, add_special_tokens=False, truncation=True, max_length=MAX_TOKENS - 5, padding=\"longest\"\n","        )[\"input_ids\"]\n","\n","        # 🔹 Directly get sentence lengths from tokenized output\n","        sentence_lengths = [len(tokens) for tokens in sentence_tokens]\n","\n","        # No need to wrap this inner loop with tqdm\n","        for sentence, sentence_tokenized, sentence_length in zip(sentences, sentence_tokens, sentence_lengths):\n","            if token_count + sentence_length > MAX_TOKENS - 10:\n","                chunk_text = \" \".join(current_chunk)\n","\n","                # 🔹 Tokenize chunk in one step (without looping)\n","                tokenized_chunk = tokenizer.encode(\n","                    chunk_text, add_special_tokens=True, truncation=True, max_length=MAX_TOKENS - 2, padding=\"max_length\"\n","                )[:MAX_TOKENS]  # 🔹 Hard truncate before storing\n","\n","                chunks.append(tokenized_chunk)\n","                total_chunks += 1\n","\n","                current_chunk = current_chunk[-(OVERLAP // 2):]\n","                token_count = sum(sentence_lengths[-(OVERLAP // 2):])\n","\n","            current_chunk.append(sentence)\n","            token_count += sentence_length\n","\n","        if current_chunk:\n","            chunk_text = \" \".join(current_chunk)\n","\n","            # 🔹 Tokenize final chunk\n","            tokenized_chunk = tokenizer.encode(\n","                chunk_text, add_special_tokens=True, truncation=True, max_length=MAX_TOKENS - 2, padding=\"max_length\"\n","            )[:MAX_TOKENS]\n","\n","            chunks.append(tokenized_chunk)\n","            total_chunks += 1\n","\n","        chunked_docs[doc_id] = chunks  # Store document chunks\n","\n","    return chunked_docs\n","\n","# 🔹 Step 3: Attach Questions to Chunks\n","def generate_question_context_pairs(contracts_data, chunked_docs):\n","    \"\"\"Pairs each question with the pre-chunked document context.\"\"\"\n","    chunked_dataset = []\n","    # Wrap the loop with tqdm for progress tracking\n","    for contract in tqdm(contracts_data, desc=\"Generating question-context pairs\"):\n","        doc_id = contract[\"doc_id\"]\n","        clause_type = contract[\"clause_type\"]\n","        expected_output = contract[\"expected_output\"]\n","\n","        if doc_id not in chunked_docs:\n","            continue  # Skip if the document isn't chunked (shouldn't happen)\n","        # Wrap the loop with tqdm for progress tracking within contract processing\n","        for chunk in tqdm(chunked_docs[doc_id], desc=\"Processing Chunks\", leave=False):\n","            # Check if this chunk contains the answer (Token-Based Answer Matching)\n","            tokenized_answers = [tokenizer.encode(ans, add_special_tokens=False) for ans in expected_output]\n","            answer_present = any(all(token in chunk for token in tokenized_answer) for tokenized_answer in tokenized_answers)\n","\n","            # Light filtering: Keep 33% of non-answer chunks\n","            # if not answer_present and random.random() > 0.33:  # You might have removed random earlier, if not, import it\n","            #     continue\n","\n","            formatted_input = f\"question: {clause_type} context: {tokenizer.decode(chunk, skip_special_tokens=True)}\"\n","\n","            chunked_dataset.append({\n","                \"doc_id\": doc_id,\n","                \"input\": formatted_input,\n","                \"expected_output\": expected_output if answer_present else [],\n","                \"answer_present\": answer_present\n","            })\n","\n","    return chunked_dataset\n","\n","def process_all_contracts(contracts_data):\n","    \"\"\"Processes all contracts efficiently without redundant document processing.\"\"\"\n","    print(\"Extracting unique documents...\")\n","    doc_map = extract_unique_documents(contracts_data)\n","\n","    print(\"Chunking documents...\")\n","    chunked_docs = chunk_documents(doc_map)\n","\n","    print(\"Generating question-context pairs...\")\n","    chunked_dataset = generate_question_context_pairs(contracts_data, chunked_docs)\n","\n","    return chunked_dataset\n","\n","if __name__ == \"__main__\":\n","    print(\"Processing dataset with optimized chunking and answer prioritization (one-pass mode)...\")\n","    chunked_dataset = process_all_contracts(contracts_data)\n","\n","    print(f\"Processing complete. Total chunks: {len(chunked_dataset)}\")"],"metadata":{"id":"wuowC0h-IHq3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer\n","\n","# 🔹 Load preprocessed contracts_data (our new dataset)\n","contracts_data = contracts_data # This should be the cleaned dataset\n","\n","# 🔹 Load tokenizer and move to GPU if available\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model_name = \"google/flan-t5-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n","MAX_TOKENS = 512\n","OVERLAP = 256\n","# 🔹 Step 1: Extract unique documents (Avoid redundant tokenization)\n","\n","def extract_unique_documents(contracts_data):\n","    \"\"\"Creates a dictionary mapping each unique document ID to its full text.\"\"\"\n","    doc_map = {}\n","    for contract in contracts_data:\n","        doc_id = contract[\"doc_id\"]\n","        if doc_id not in doc_map:\n","            doc_map[doc_id] = contract[\"input\"].split(\"CONTEXT: \")[1].strip()  # Extract only document text\n","    return doc_map\n","\n","def chunk_documents(doc_map):\n","    \"\"\"Chunks each document efficiently using batch tokenization.\"\"\"\n","    chunked_docs = {}\n","\n","    for doc_id, text in doc_map.items():\n","        text = text.replace(\"\\n\", \" \").strip()  # Remove unnecessary newlines & spaces\n","        sentences = text.split(\". \")  # Keep sentence boundaries\n","\n","        chunks = []\n","        current_chunk = []\n","        token_count = 0\n","        total_chunks = 0\n","\n","        # 🔹 Tokenize sentences in BATCHES (GPU-accelerated)\n","        sentence_tokens = tokenizer.batch_encode_plus(\n","            sentences, add_special_tokens=False, truncation=True, max_length=MAX_TOKENS - 5, padding=\"longest\"\n","        )[\"input_ids\"]\n","\n","        # 🔹 Directly get sentence lengths from tokenized output\n","        sentence_lengths = [len(tokens) for tokens in sentence_tokens]\n","\n","        for sentence, sentence_tokenized, sentence_length in zip(sentences, sentence_tokens, sentence_lengths):\n","            if token_count + sentence_length > MAX_TOKENS - 10:\n","                chunk_text = \" \".join(current_chunk)\n","\n","                # 🔹 Tokenize chunk in one step (without looping)\n","                tokenized_chunk = tokenizer.encode(\n","                    chunk_text, add_special_tokens=True, truncation=True, max_length=MAX_TOKENS - 2, padding=\"max_length\"\n","                )[:MAX_TOKENS]  # 🔹 Hard truncate before storing\n","\n","                chunks.append(tokenized_chunk)\n","                total_chunks += 1\n","\n","                current_chunk = current_chunk[-(OVERLAP // 2):]\n","                token_count = sum(sentence_lengths[-(OVERLAP // 2):])\n","\n","            current_chunk.append(sentence)\n","            token_count += sentence_length\n","\n","        if current_chunk:\n","            chunk_text = \" \".join(current_chunk)\n","\n","            # 🔹 Tokenize final chunk\n","            tokenized_chunk = tokenizer.encode(\n","                chunk_text, add_special_tokens=True, truncation=True, max_length=MAX_TOKENS - 2, padding=\"max_length\"\n","            )[:MAX_TOKENS]\n","\n","            chunks.append(tokenized_chunk)\n","            total_chunks += 1\n","\n","        chunked_docs[doc_id] = chunks  # Store document chunks\n","\n","    return chunked_docs\n","# 🔹 Step 3: Attach Questions to Chunks\n","def generate_question_context_pairs(contracts_data, chunked_docs):\n","    \"\"\"Pairs each question with the pre-chunked document context.\"\"\"\n","    chunked_dataset = []\n","\n","    for contract in contracts_data:\n","        doc_id = contract[\"doc_id\"]\n","        clause_type = contract[\"clause_type\"]\n","        expected_output = contract[\"expected_output\"]\n","\n","        if doc_id not in chunked_docs:\n","            continue  # Skip if the document isn't chunked (shouldn't happen)\n","\n","        for chunk in chunked_docs[doc_id]:\n","            # Check if this chunk contains the answer (Token-Based Answer Matching)\n","            tokenized_answers = [tokenizer.encode(ans, add_special_tokens=False) for ans in expected_output]\n","            answer_present = any(all(token in chunk for token in tokenized_answer) for tokenized_answer in tokenized_answers)\n","\n","            # Light filtering: Keep 33% of non-answer chunks\n","            if not answer_present and random.random() > 0.33:\n","                continue\n","\n","            formatted_input = f\"question: {clause_type} context: {tokenizer.decode(chunk, skip_special_tokens=True)}\"\n","\n","            chunked_dataset.append({\n","                \"doc_id\": doc_id,\n","                \"input\": formatted_input,\n","                \"expected_output\": expected_output if answer_present else [],\n","                \"answer_present\": answer_present\n","            })\n","\n","    return chunked_dataset\n","\n","def process_all_contracts(contracts_data):\n","    \"\"\"Processes all contracts efficiently without redundant document processing.\"\"\"\n","    print(\"Extracting unique documents...\")\n","    doc_map = extract_unique_documents(contracts_data)\n","\n","    print(\"Chunking documents...\")\n","    chunked_docs = chunk_documents(doc_map)\n","\n","    print(\"Generating question-context pairs...\")\n","    chunked_dataset = generate_question_context_pairs(contracts_data, chunked_docs)\n","\n","    return chunked_dataset\n","\n","if __name__ == \"__main__\":\n","    print(\"Processing dataset with optimized chunking and answer prioritization (one-pass mode)...\")\n","    chunked_dataset = process_all_contracts(contracts_data)\n","\n","    print(f\"Processing complete. Total chunks: {len(chunked_dataset)}\")"],"metadata":{"id":"QI5l4vjXLvMq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import pandas as pd\n","import re\n","import torch\n","import random\n","from transformers import AutoTokenizer\n","from tqdm import tqdm  # Import tqdm for progress tracking\n","\n","# 🔹 Load preprocessed contracts_data (our new dataset)\n","contracts_data = contracts_data  # This should be the cleaned dataset\n","\n","# 🔹 Load tokenizer and move to GPU if available\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model_name = \"google/flan-t5-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n","MAX_TOKENS = 512\n","OVERLAP = 256\n","\n","# 🔹 Step 1: Extract unique documents (Avoid redundant tokenization)\n","def extract_unique_documents(contracts_data):\n","    \"\"\"Creates a dictionary mapping each unique document ID to its full text.\"\"\"\n","    doc_map = {}\n","    # Wrap the loop with tqdm for progress tracking\n","    for contract in tqdm(contracts_data, desc=\"Extracting unique documents\"):\n","        doc_id = contract[\"doc_id\"]\n","        if doc_id not in doc_map:\n","            doc_map[doc_id] = contract[\"input\"].split(\"CONTEXT: \")[1].strip()  # Extract only document text\n","    return doc_map\n","\n","def chunk_documents(doc_map):\n","    \"\"\"Chunks each document efficiently using batch tokenization.\"\"\"\n","    chunked_docs = {}\n","\n","    # Wrap the outer loop with tqdm for progress tracking\n","    for doc_id, text in tqdm(doc_map.items(), desc=\"Chunking documents\"):\n","        text = text.replace(\"\\n\", \" \").strip()  # Remove unnecessary newlines & spaces\n","        sentences = text.split(\". \")  # Keep sentence boundaries\n","\n","        chunks = []\n","        current_chunk = []\n","        token_count = 0\n","\n","        # 🔹 Tokenize sentences in BATCHES (GPU-accelerated)\n","        sentence_tokens = tokenizer.batch_encode(sentences, add_special_tokens=False, max_length=MAX_TOKENS - 5, truncation=True)[\"input_ids\"]\n","\n","        # 🔹 Directly get sentence lengths from tokenized output\n","        sentence_lengths = [len(tokens) for tokens in sentence_tokens]\n","\n","        # No need to wrap this inner loop with tqdm\n","        for sentence, sentence_tokenized, sentence_length in zip(sentences, sentence_tokens, sentence_lengths):\n","            if token_count + sentence_length > MAX_TOKENS - 10:\n","                chunk_text = \" \".join(current_chunk)\n","\n","                # 🔹 Tokenize chunk in one step (without looping)\n","                tokenized_chunk = tokenizer.encode(\n","                    chunk_text, add_special_tokens=True, max_length=MAX_TOKENS - 2, truncation=True)[:MAX_TOKENS]  # 🔹 Hard truncate before storing\n","\n","                chunks.append(tokenized_chunk)\n","\n","                #Contextual Overlap** (Optimized: Avoid recomputing token lengths)\n","                current_chunk = current_chunk[-(OVERLAP // 2):]\n","                token_count = sum(sentence_lengths[-(OVERLAP // 2):])\n","\n","            # Add sentence to chunk\n","            current_chunk.append(sentence)\n","            token_count += sentence_length\n","\n","        if current_chunk:\n","            chunk_text = \" \".join(current_chunk)\n","            # 🔹 Tokenize final chunk\n","            tokenized_chunk = tokenizer.encode(chunk_text, add_special_tokens=True, max_length=MAX_TOKENS - 2,truncation=True)[:MAX_TOKENS]\n","            chunks.append(tokenized_chunk)\n","\n","    return doc_id, chunks\n","\n","def chunk_documents(doc_map):\n","    \"\"\"Chunks documents using multiprocessing for faster processing.\"\"\"\n","    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n","        chunked_docs = dict(tqdm(pool.imap(chunk_document, doc_map.items()), total=len(doc_map), desc=\"Chunking documents\"))\n","    return chunked_docs\n","\n","# 🔹 Step 3: Attach Questions to Chunks\n","def generate_question_context_pairs(contracts_data, chunked_docs):\n","    \"\"\"Pairs each question with the pre-chunked document context.\"\"\"\n","    chunked_dataset = []\n","    # Wrap the loop with tqdm for progress tracking\n","    for contract in tqdm(contracts_data, desc=\"Generating question-context pairs\"):\n","        doc_id = contract[\"doc_id\"]\n","        clause_type = contract[\"clause_type\"]\n","        expected_output = contract[\"expected_output\"]\n","\n","        if doc_id not in chunked_docs:\n","            continue  # Skip if the document isn't chunked (shouldn't happen)\n","\n","        # Wrap the loop with tqdm for progress tracking within contract processing\n","        for chunk in tqdm(chunked_docs[doc_id], desc=\"Processing Chunks\", leave=True):\n","            # Check if this chunk contains the answer (Token-Based Answer Matching) with strict truncation\n","            tokenized_answers = [tokenizer.encode(ans, add_special_tokens=False, truncation=True, max_length=MAX_TOKENS - 10) for ans in expected_output] #truncation=True, max_length=MAX_TOKENS - 10)\n","            # Convert chunk into a set for fast token lookup\n","            chunk_token_set = set(chunk)  # Using set for O(1) lookups\n","            # Use set intersection for efficient answer matching instead of loop\n","            answer_present = any(set(tokenized_answer).intersection(chunk_token_set) for tokenized_answer in tokenized_answers)\n","\n","\n","            # Light filtering: Keep 33% of non-answer chunks\n","            if not answer_present and random.random() > 0.33:\n","                 continue\n","\n","            #leave decoding\n","            formatted_input = f\"question: {clause_type} context: {tokenizer.decode(chunk, skip_special_tokens=True)}\"\n","\n","            chunked_dataset.append({\n","                \"doc_id\": doc_id,\n","                \"input\": formatted_input,\n","                \"expected_output\": expected_output if answer_present else [],\n","                \"answer_present\": answer_present\n","            })\n","\n","    return chunked_dataset\n","\n","def process_all_contracts(contracts_data):\n","    \"\"\"Processes all contracts efficiently without redundant document processing.\"\"\"\n","    print(\"Extracting unique documents...\")\n","    doc_map = extract_unique_documents(contracts_data)\n","\n","    print(\"Chunking documents...\")\n","    chunked_docs = chunk_documents(doc_map)\n","\n","    print(\"Generating question-context pairs...\")\n","    chunked_dataset = generate_question_context_pairs(contracts_data, chunked_docs)\n","\n","    return chunked_dataset\n","\n","if __name__ == \"__main__\":\n","    print(\"Processing dataset with optimized chunking and answer prioritization (one-pass mode)...\")\n","    chunked_dataset = process_all_contracts(contracts_data)\n","\n","    print(f\"Processing complete. Total chunks: {len(chunked_dataset)}\")"],"metadata":{"id":"dB4Zn9rfRuY-","executionInfo":{"status":"ok","timestamp":1742335514763,"user_tz":240,"elapsed":22,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["Keep this. This is being used in untitled notebook 3. I changed the logic in that one to include check point saving but if it doesn't work, use this instead. just cross check logic and comments"],"metadata":{"id":"Mc_RtZiUs2Se"}},{"cell_type":"code","source":["import json\n","import pandas as pd\n","import re\n","import torch\n","import random\n","import multiprocessing\n","from transformers import AutoTokenizer\n","from tqdm import tqdm\n","\n","# 🔹 Load preprocessed contracts_data (our new dataset)\n","contracts_data = contracts_data  # Ensure this is defined before processing\n","\n","# 🔹 Load tokenizer and move to GPU if available\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model_name = \"google/flan-t5-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n","MAX_TOKENS = 512\n","OVERLAP = 256\n","\n","# 🔹 Step 1: Extract unique documents (Avoid redundant tokenization)\n","def extract_unique_documents(contracts_data):\n","    \"\"\"Creates a dictionary mapping each unique document ID to its full text.\"\"\"\n","    doc_map = {}\n","    for contract in tqdm(contracts_data, desc=\"Extracting unique documents\"):\n","        doc_id = contract[\"doc_id\"]\n","        if doc_id not in doc_map:\n","            doc_map[doc_id] = contract[\"input\"].split(\"CONTEXT: \", 1)[-1].strip()  # Extract only document text\n","    return doc_map\n","\n","def chunk_document(doc_id_text_pair):\n","    \"\"\"Chunks a single document efficiently.\n","    Chunks each document efficiently using batch tokenization.\"\"\"\n","    doc_id, text = doc_id_text_pair\n","    text = text.replace(\"\\n\", \" \").strip()  # Remove unnecessary newlines & spaces\n","    sentences = text.split(\". \")  # Keep sentence boundaries\n","\n","    chunks = []\n","    current_chunk = []\n","    token_count = 0\n","\n","    # 🔹 Batch tokenize sentences in one step (GPU-accelerated)\n","    sentence_tokens = tokenizer.batch_encode(sentences, add_special_tokens=False, max_length=MAX_TOKENS - 5, truncation=True)\n","\n","    # 🔹 Get sentence lengths directly\n","    sentence_lengths = [len(tokens) for tokens in sentence_tokens]\n","\n","    for sentence, sentence_tokenized, sentence_length in zip(sentences, sentence_tokens, sentence_lengths):\n","        if token_count + sentence_length > MAX_TOKENS - 10:\n","            chunk_text = \" \".join(current_chunk)\n","\n","            # 🔹 Tokenize chunk in one step (efficient truncation)\n","            tokenized_chunk = tokenizer.encode(\n","                chunk_text, add_special_tokens=True, max_length=MAX_TOKENS - 2, truncation=True\n","            )[:MAX_TOKENS]\n","\n","            chunks.append(tokenized_chunk)\n","\n","            # 🔹 Overlap handling (efficient slicing)\n","            current_chunk = current_chunk[-(OVERLAP // 2):]\n","            token_count = sum(sentence_lengths[-(OVERLAP // 2):])\n","\n","        # Add sentence to chunk\n","        current_chunk.append(sentence)\n","        token_count += sentence_length\n","\n","    if current_chunk:\n","        chunk_text = \" \".join(current_chunk)\n","        tokenized_chunk = tokenizer.encode(chunk_text, add_special_tokens=True, max_length=MAX_TOKENS - 2, truncation=True)[:MAX_TOKENS]\n","        chunks.append(tokenized_chunk)\n","\n","    return doc_id, chunks\n","\n","def chunk_documents(doc_map):\n","    \"\"\"Chunks documents using multiprocessing for faster processing.\"\"\"\n","    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n","        chunked_docs = dict(tqdm(pool.imap(chunk_document, doc_map.items()), total=len(doc_map), desc=\"Chunking documents\"))\n","    return chunked_docs\n","\n","# 🔹 Step 3: Attach Questions to Chunks\n","def generate_question_context_pairs(contracts_data, chunked_docs):\n","    \"\"\"Pairs each question with the pre-chunked document context.\"\"\"\n","    chunked_dataset = []\n","\n","    def process_contract(contract):\n","        \"\"\"Helper function to process a single contract.\"\"\"\n","        doc_id = contract[\"doc_id\"]\n","        clause_type = contract[\"clause_type\"]\n","        expected_output = contract[\"expected_output\"]\n","\n","        if doc_id not in chunked_docs:\n","            return [] #shouldn't happen\n","\n","        chunk_data = []\n","        # Check if this chunk contains the answer (Token-Based Answer Matching) with strict truncation\n","        tokenized_answers = [set(tokenizer.encode(ans, add_special_tokens=False, truncation=True, max_length=MAX_TOKENS - 10)) for ans in expected_output]\n","\n","        for chunk in chunked_docs[doc_id]:\n","            # Convert chunk into a set for fast token lookup\n","            chunk_token_set = set(chunk) # Using set for O(1) lookups\n","            # Use set intersection for efficient answer matching instead of loop\n","            answer_present = any(set(tokenized_answer).intersection(chunk_token_set) for tokenized_answer in tokenized_answers)\n","\n","            # Light filtering: Keep 33% of non-answer chunks\n","            if not answer_present and random.random() > 0.33:\n","                continue\n","\n","            formatted_input = f\"question: {clause_type} context: {tokenizer.decode(chunk, skip_special_tokens=True)}\"\n","\n","            chunk_data.append({\n","                \"doc_id\": doc_id,\n","                \"input\": formatted_input,\n","                \"expected_output\": expected_output if answer_present else [],\n","                \"answer_present\": answer_present\n","            })\n","        return chunk_data\n","\n","    # 🔹 Use multiprocessing for faster processing\n","    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n","        results = list(tqdm(pool.imap(process_contract, contracts_data), total=len(contracts_data), desc=\"Generating question-context pairs\"))\n","\n","    # Flatten results list\n","    for result in results:\n","        chunked_dataset.extend(result)\n","\n","    return chunked_dataset\n","\n","def process_all_contracts(contracts_data):\n","    \"\"\"Processes all contracts efficiently without redundant document processing.\"\"\"\n","    print(\"Extracting unique documents...\")\n","    doc_map = extract_unique_documents(contracts_data)\n","\n","    print(\"Chunking documents with multiprocessing...\")\n","    chunked_docs = chunk_documents(doc_map)\n","\n","    print(\"Generating question-context pairs with multiprocessing...\")\n","    chunked_dataset = generate_question_context_pairs(contracts_data, chunked_docs)\n","\n","    return chunked_dataset\n","\n","if __name__ == \"__main__\":\n","    print(\"Processing dataset with optimized chunking and answer prioritization (parallel mode)...\")\n","    chunked_dataset = process_all_contracts(contracts_data)\n","\n","    print(f\"Processing complete. Total chunks: {len(chunked_dataset)}\")\n"],"metadata":{"id":"TkLw85WqKWyB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Up2-hKR4gIDq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["How to Resume Processing If Runtime Stops (Google Colab)\n","If your Colab runtime disconnects or crashes, you don’t need to start from scratch. Just rerun the script, and it will automatically resume from the last saved checkpoint.\n","\n","Steps to Resume Processing\n","- Reconnect to Google Colab.\n","Mount Google Drive (if using Drive for storage).\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","Rerun your script (no need to modify anything).\n","chunked_dataset = process_all_contracts(contracts_data)\n","\n"," What Happens When You Restart?\n","- load_checkpoint() reads the last saved progress.\n","- It skips previously processed contracts (it starts from start_idx).\n","- Newly processed data is added without duplication.\n","\n","Example Scenario:\n","\n","Suppose your script stops at contract 1,500.\n","When restarted, it picks up from contract 1,500, not from 0.\n","Final dataset integrity is preserved.\n","\n","Additional Check (If You're Unsure)\n","Before running process_all_contracts(), you can manually check the last saved checkpoint:\n","\n","start_idx, _ = load_checkpoint()\n","print(f\"Resuming from contract {start_idx}...\")\n","\n","This prints where it will resume from before processing.\n","\n","That’s it! Just rerun the script, and it will resume exactly where it left off\n"],"metadata":{"id":"S33TG6HbgKCa"}}]}