{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOOrtdIOePHIXkeZFChbjr8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LLMe8SHL-2Qc"},"outputs":[],"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","from nltk.tokenize import sent_tokenize\n","\n","def split_into_chunks(text, max_tokens, overlap, tokenizer, question, return_text=False):\n","    \"\"\"\n","    Splits text into logical 512-token chunks using paragraph & clause boundaries.\n","    Ensures no chunk exceeds the model's max length.\n","    \"\"\"\n","\n","    paragraphs = text.split(\"\\n\\n\")  # Step 1: Split by paragraph\n","    chunks = []\n","    current_chunk = []\n","    token_count = 0\n","\n","    for paragraph in paragraphs:\n","        sentences = sent_tokenize(paragraph)\n","        paragraph_tokens = tokenizer.encode(paragraph, add_special_tokens=False)\n","\n","        # Step 2: Merge sentences into logical units\n","        for sentence in sentences:\n","            sentence_tokens = tokenizer.encode(sentence, add_special_tokens=False)\n","            sentence_length = len(sentence_tokens)\n","\n","            # If adding this sentence exceeds max tokens, finalize the chunk\n","            if token_count + sentence_length > max_tokens - 50:  # 50 reserved for \"question: ...\" prefix\n","                chunk_text = \" \".join(current_chunk)\n","                formatted_text = f\"question: {question} context: {chunk_text}\"\n","\n","                # Step 3: Truncate full formatted input to 512 tokens\n","                formatted_chunk = tokenizer.encode(formatted_text, add_special_tokens=True, truncation=True, max_length=max_tokens)\n","\n","                if return_text:\n","                    chunks.append(tokenizer.decode(formatted_chunk, skip_special_tokens=False))\n","                else:\n","                    chunks.append(formatted_chunk)\n","\n","                # Step 4: Contextual overlap (keep full logical units)\n","                current_chunk = current_chunk[-(overlap // 2):]\n","                token_count = sum(len(tokenizer.encode(sent, add_special_tokens=False)) for sent in current_chunk)\n","\n","            # Add sentence to chunk\n","            current_chunk.append(sentence)\n","            token_count += sentence_length\n","\n","    # Step 5: Add last chunk\n","    if current_chunk:\n","        chunk_text = \" \".join(current_chunk)\n","        formatted_text = f\"question: {question} context: {chunk_text}\"\n","        formatted_chunk = tokenizer.encode(formatted_text, add_special_tokens=True, truncation=True, max_length=max_tokens)\n","\n","        if return_text:\n","            chunks.append(tokenizer.decode(formatted_chunk, skip_special_tokens=False))\n","        else:\n","            chunks.append(formatted_chunk)\n","\n","    return chunks\n"]},{"cell_type":"markdown","source":["second tokenizer- faster than above but trying to move to gpu"],"metadata":{"id":"tTSmtKbhC-Ct"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize\n","\n","def split_into_chunks(text, max_tokens, overlap, tokenizer, question, return_text=False):\n","    \"\"\"\n","    Efficiently splits text into logical 512-token chunks while preserving clause and paragraph boundaries.\n","    \"\"\"\n","\n","    paragraphs = text.split(\"\\n\\n\")  # Step 1: Split by paragraph\n","    chunks = []\n","    current_chunk = []\n","    token_count = 0\n","\n","    for paragraph in paragraphs:\n","        sentences = sent_tokenize(paragraph)\n","\n","        # Step 2: Tokenize all sentences at once (Optimized)\n","        sentence_tokens = tokenizer.batch_encode_plus(sentences, add_special_tokens=False)['input_ids']\n","        sentence_lengths = [len(tokens) for tokens in sentence_tokens]  # Cache token lengths\n","\n","        # Step 3: Merge sentences efficiently\n","        for sentence, sentence_tokenized, sentence_length in zip(sentences, sentence_tokens, sentence_lengths):\n","\n","            # If adding this sentence exceeds max tokens, finalize the chunk\n","            if token_count + sentence_length > max_tokens - 50:  # 50 reserved for prefix\n","                chunk_text = \" \".join(current_chunk)\n","                formatted_text = f\"question: {question} context: {chunk_text}\"\n","\n","                # Truncate final chunk\n","                formatted_chunk = tokenizer.encode(formatted_text, add_special_tokens=True, truncation=True, max_length=max_tokens)\n","\n","                chunks.append(tokenizer.decode(formatted_chunk) if return_text else formatted_chunk)\n","\n","                # **Step 4: Contextual Overlap** (Optimized: Avoid recomputing token lengths)\n","                current_chunk = current_chunk[-(overlap // 2):]\n","                token_count = sum(sentence_lengths[-(overlap // 2):])\n","\n","            # Add sentence to chunk\n","            current_chunk.append(sentence)\n","            token_count += sentence_length\n","\n","    # Step 5: Add last chunk\n","    if current_chunk:\n","        chunk_text = \" \".join(current_chunk)\n","        formatted_text = f\"question: {question} context: {chunk_text}\"\n","        formatted_chunk = tokenizer.encode(formatted_text, add_special_tokens=True, truncation=True, max_length=max_tokens)\n","\n","        chunks.append(tokenizer.decode(formatted_chunk) if return_text else formatted_chunk)\n","\n","    return chunks\n"],"metadata":{"id":"B4OOoEsADDN_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to assign a confidence score (New)\n","def compute_confidence(chunk_tokens, answers, tokenizer):\n","    \"\"\"\n","    Computes confidence score based on token-level answer presence.\n","\n","    Parameters:\n","    - chunk_tokens (list[int]): Tokenized chunk (list of token IDs).\n","    - answers (list[str]): List of possible correct answers.\n","    - tokenizer (AutoTokenizer): Tokenizer used for encoding.\n","\n","    Returns:\n","    - confidence_score (float): Ranges from 0.0 (no match) to 1.0 (perfect match).\n","    \"\"\"\n","\n","    if not answers:\n","        return 0.0  # No answer should be found\n","\n","    # Tokenize each answer to compare at the token level\n","    tokenized_answers = [tokenizer.encode(ans, add_special_tokens=False) for ans in answers]\n","\n","    exact_matches = 0\n","    partial_matches = 0\n","\n","    for tokenized_answer in tokenized_answers:\n","        if tokenized_answer in chunk_tokens:\n","            exact_matches += 1  # Perfect match (entire answer found)\n","        else:\n","            # Check if part of the tokenized answer appears in the chunk\n","            partial_count = sum(1 for token in tokenized_answer if token in chunk_tokens)\n","            if partial_count > 0:\n","                partial_matches += partial_count / len(tokenized_answer)  # Partial match ratio\n","\n","    # Compute final confidence score\n","    if exact_matches == len(answers):\n","        return 1.0  # Perfect match for all answers\n","    elif partial_matches > 0:\n","        return round(partial_matches / len(answers), 2)  # Partial confidence\n","    return 0.0  # No match found"],"metadata":{"id":"pOKp019W_C_A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["moving compute confidence to gpu but this is faster than above"],"metadata":{"id":"dAwAQ2T7Dg8_"}},{"cell_type":"code","source":["def compute_confidence(chunk_tokens, answers, tokenizer):\n","    \"\"\"\n","    Computes confidence score based on token-level answer presence.\n","\n","    Optimizations:\n","    - Uses a set for `chunk_tokens` (O(1) lookup instead of O(N))\n","    - Uses `set.intersection()` for fast partial match checking\n","    - Improves confidence calculation by using max recall instead of averaging\n","\n","    Returns:\n","    - confidence_score (float): Ranges from 0.0 (no match) to 1.0 (perfect match).\n","    \"\"\"\n","\n","    if not answers:\n","        return 0.0  # No answer should be found\n","\n","    # Tokenize each answer\n","    tokenized_answers = [tokenizer.encode(ans, add_special_tokens=False) for ans in answers]\n","\n","    # Convert chunk tokens to a set for faster lookups\n","    chunk_token_set = set(chunk_tokens)\n","\n","    exact_matches = 0\n","    partial_match_ratios = []\n","\n","    for tokenized_answer in tokenized_answers:\n","        answer_set = set(tokenized_answer)\n","\n","        if answer_set.issubset(chunk_token_set):  # Perfect match\n","            exact_matches += 1\n","        else:\n","            # Compute partial match ratio\n","            overlap = len(answer_set.intersection(chunk_token_set))\n","            partial_match_ratios.append(overlap / len(answer_set) if len(answer_set) > 0 else 0)\n","\n","    # Compute final confidence score\n","    if exact_matches == len(answers):\n","        return 1.0  # All answers perfectly matched\n","    elif partial_match_ratios:\n","        return round(max(partial_match_ratios), 2)  # Take the highest recall ratio\n","    return 0.0  # No match found\n"],"metadata":{"id":"XVYmgFNMDual"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["old preprocess data"],"metadata":{"id":"MQRCluq1Fb4i"}},{"cell_type":"code","source":["import json\n","from multiprocessing import Pool\n","from tqdm import tqdm\n","\n","# Load CUAD dataset\n","with open('/content/drive/My Drive/Colab Notebooks/CUAD_v1.json', 'r', encoding='utf-8') as file:\n","    dataset = json.load(file)[\"data\"]\n","\n","# Function to process a single contract\n","def process_contract(contract):\n","    processed_contract_data = []\n","    contract_title = clean_text(contract[\"title\"])\n","    doc_id = contract[\"title\"]\n","\n","    for paragraph in contract[\"paragraphs\"]:\n","        context = clean_text(paragraph[\"context\"])\n","\n","        for qa in paragraph[\"qas\"]:\n","            clause_type = clean_text(qa[\"question\"].split(\"related to \\\"\")[1].split(\"\\\"\")[0])\n","            answers = [clean_text(ans[\"text\"]) for ans in qa[\"answers\"]] if not qa[\"is_impossible\"] else []\n","\n","            # Generate refined instruction prompt\n","            instruction_prompt = generate_instruction_prompt(clause_type, context)\n","\n","            # Split into chunks\n","            chunks = split_into_chunks(context, MAX_TOKENS, OVERLAP, tokenizer, clause_type)\n","\n","            for chunk in chunks:\n","                # Compute confidence score\n","                confidence = compute_answer_presence(chunk, answers, tokenizer)\n","                answer_present = confidence > 0\n","\n","                # Store structured data\n","                processed_contract_data.append({\n","                    \"doc_id\": doc_id,\n","                    \"input\": instruction_prompt,\n","                    \"expected_output\": answers if answer_present else [],\n","                    \"answer_present\": answer_present,\n","                    \"confidence_score\": confidence\n","                })\n","\n","    return processed_contract_data\n","\n","# Use multiprocessing with 8 workers and tqdm progress bar\n","if __name__ == \"__main__\":\n","    print(\"Starting dataset processing with 8 workers...\\n\")\n","\n","    with Pool(processes=8) as pool:\n","        results = []\n","        with tqdm(total=len(dataset)) as pbar:\n","            for result in pool.imap_unordered(process_contract, dataset):\n","                results.append(result)\n","                pbar.update(1)\n","\n","    # Flatten the results\n","    processed_data = [item for sublist in results for item in sublist]\n","\n","    print(\"\\nDataset processing complete! Saving results...\")\n","\n","    # Save processed dataset\n","    save_path = \"/content/drive/My Drive/Colab Notebooks/NLP_266_Project/chunked_datasets/enhanced_chunking_dataset.json\"\n","    with open(save_path, \"w\", encoding=\"utf-8\") as file:\n","        json.dump(processed_data, file, indent=4)\n","\n","    print(f\" Saved {len(processed_data)} processed examples to {save_path}\")\n"],"metadata":{"id":"-2ZUWtjgFbf7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer\n","\n","# Load tokenizer and move to GPU if available\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model_name = \"google/flan-t5-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n","\n","\n","# Chunking parameters\n","MAX_TOKENS = 512  # Larger context for legal clauses\n","OVERLAP = 256  # More overlap to avoid clause splits\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize\n","\n","def split_into_chunks(text, max_tokens, overlap, tokenizer, question, return_text=False):\n","    \"\"\"\n","    Efficiently splits text into logical 512-token chunks while preserving clause and paragraph boundaries.\n","    Moves tokenization to GPU for faster processing.\n","    \"\"\"\n","\n","    paragraphs = text.split(\"\\n\\n\")  # Step 1: Split by paragraph\n","    chunks = []\n","    current_chunk = []\n","    token_count = 0\n","\n","    for paragraph in paragraphs:\n","        sentences = sent_tokenize(paragraph)\n","\n","        # Step 2: Tokenize all sentences at once and move to GPU\n","        sentence_tokens = tokenizer.batch_encode_plus(sentences, add_special_tokens=False)['input_ids']\n","        sentence_tokens = [torch.tensor(tokens, device=device) for tokens in sentence_tokens]  # Move to GPU\n","        sentence_lengths = [len(tokens) for tokens in sentence_tokens]  # Cache token lengths\n","\n","        # Step 3: Merge sentences efficiently\n","        for sentence, sentence_tokenized, sentence_length in zip(sentences, sentence_tokens, sentence_lengths):\n","\n","            # If adding this sentence exceeds max tokens, finalize the chunk\n","            if token_count + sentence_length > max_tokens - 50:  # 50 reserved for prefix\n","                chunk_text = \" \".join(current_chunk)\n","                formatted_text = f\"question: {question} context: {chunk_text}\"\n","\n","                # Truncate final chunk and move to GPU\n","                formatted_chunk = tokenizer.encode(formatted_text, add_special_tokens=True, truncation=True, max_length=max_tokens)\n","                formatted_chunk = torch.tensor(formatted_chunk, device=device)  # Move to GPU\n","\n","                chunks.append(tokenizer.decode(formatted_chunk.tolist()) if return_text else formatted_chunk.tolist())\n","\n","                # **Step 4: Contextual Overlap** (Optimized: Avoid recomputing token lengths)\n","                current_chunk = current_chunk[-(overlap // 2):]\n","                token_count = sum(sentence_lengths[-(overlap // 2):])\n","\n","            # Add sentence to chunk\n","            current_chunk.append(sentence)\n","            token_count += sentence_length\n","\n","    # Step 5: Add last chunk\n","    if current_chunk:\n","        chunk_text = \" \".join(current_chunk)\n","        formatted_text = f\"question: {question} context: {chunk_text}\"\n","        formatted_chunk = tokenizer.encode(formatted_text, add_special_tokens=True, truncation=True, max_length=max_tokens)\n","        formatted_chunk = torch.tensor(formatted_chunk, device=device)  # Move to GPU\n","\n","        chunks.append(tokenizer.decode(formatted_chunk.tolist()) if return_text else formatted_chunk.tolist())\n","\n","    return chunks\n","\n","\n","def clean_text(text):\n","    \"\"\"Cleans text by removing extra spaces and newlines.\"\"\"\n","    text = text.strip()\n","    text = re.sub(r'\\s+', ' ', text)\n","    return text\n","\n","\n","\n","def generate_instruction_prompt(clause_type, context):\n","    return (\n","        f\"You are a legal document analyst specializing in contract clause identification. \"\n","        f\"Your task is to accurately extract the {clause_type} from the provided text. \"\n","        f\"Follow these steps carefully:\\n\"\n","        f\"1. Identify the exact clause without adding or removing words.\\n\"\n","        f\"2. If the {clause_type} is missing or is not explicitly present, return [].\\n\"\n","        f\"3. Ensure no unrelated text is included in the extraction.\\n\"\n","        f\"4. If only part of the clause appears, extract the full statement if possible; otherwise, return [].\\n\"\n","        f\"5. Do not make assumptions or interpret missing text—strictly return what is present.\\n\"\n","        f\"---\\n\"\n","        f\"QUESTION: {clause_type}\\n\"\n","        f\"CONTEXT: {context}\"\n","    )\n","\n","\n","import torch\n","\n","def compute_answer_presence(chunk_tokens, answers, tokenizer):\n","    \"\"\"\n","    Checks if any answer is fully present in the chunk.\n","    Optimized for GPU processing to reduce computation time.\n","\n","    Optimizations:\n","    - Uses PyTorch tensors for fast token comparisons\n","    - Returns early if an exact match is found (saves time)\n","    - Removes unnecessary computations for partial matches\n","\n","    Returns:\n","    - answer_present (bool): True if any answer is found in chunk, else False\n","    \"\"\"\n","\n","    if not answers:\n","        return False  # No answer should be found\n","\n","    # Move chunk tokens to GPU\n","    chunk_tokens = torch.tensor(chunk_tokens, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Tokenize answers and move to GPU\n","    tokenized_answers = [torch.tensor(tokenizer.encode(ans, add_special_tokens=False),\n","                                      device=chunk_tokens.device) for ans in answers]\n","\n","    for tokenized_answer in tokenized_answers:\n","        # If entire answer tokens exist within chunk tokens, return True immediately\n","        if all(token in chunk_tokens for token in tokenized_answer):\n","            return True  # Answer found, exit early\n","\n","    return False  # No answer found\n","\n","# Load CUAD dataset\n","with open('/content/drive/My Drive/Colab Notebooks/CUAD_v1.json', 'r', encoding='utf-8') as file:\n","    dataset = json.load(file)[\"data\"]\n","\n","\n","# Function to process a single contract\n","def process_contract(contract):\n","    processed_contract_data = []\n","    contract_title = clean_text(contract[\"title\"])\n","    doc_id = contract[\"title\"]\n","\n","    for paragraph in contract[\"paragraphs\"]:\n","        context = clean_text(paragraph[\"context\"])\n","\n","        for qa in paragraph[\"qas\"]:\n","            clause_type = clean_text(qa[\"question\"].split(\"related to \\\"\")[1].split(\"\\\"\")[0])\n","            answers = [clean_text(ans[\"text\"]) for ans in qa[\"answers\"]] if not qa[\"is_impossible\"] else []\n","\n","            # Generate refined instruction prompt\n","            instruction_prompt = generate_instruction_prompt(clause_type, context)\n","\n","            # Split into chunks (now moved to GPU)\n","            chunks = split_into_chunks(context, MAX_TOKENS, OVERLAP, tokenizer, clause_type)\n","\n","            for chunk in chunks:\n","                # Compute answer presence (Binary flag, optimized on GPU)\n","                answer_present = compute_answer_presence(chunk, answers, tokenizer)\n","\n","                # Store structured data\n","                processed_contract_data.append({\n","                    \"doc_id\": doc_id,\n","                    \"input\": instruction_prompt,\n","                    \"expected_output\": answers if answer_present else [],\n","                    \"answer_present\": answer_present,\n","                })\n","\n","    return processed_contract_data"],"metadata":{"id":"ehhqx3PBLNNa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import multiprocessing\n","import psutil\n","\n","print(\"CPUs available:\", multiprocessing.cpu_count())\n","print(\"RAM available (GB):\", round(psutil.virtual_memory().total / 1e9, 2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G4hJAU57gtPj","executionInfo":{"status":"ok","timestamp":1742960108553,"user_tz":240,"elapsed":22,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}},"outputId":"6acf9e39-7fa8-4b50-ee9e-7f35277e7853"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["CPUs available: 12\n","RAM available (GB): 89.63\n"]}]},{"cell_type":"code","source":["#check gpu\n","!nvidia-smi\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tLKVbMPihYQV","executionInfo":{"status":"ok","timestamp":1742960169798,"user_tz":240,"elapsed":266,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}},"outputId":"afd70fa7-a5ce-49df-8161-ee06904d21db"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Mar 26 03:36:09 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   33C    P0             44W /  400W |       0MiB /  40960MiB |      0%      Default |\n","|                                         |                        |             Disabled |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["import torch\n","\n","print(\"GPU available:\", torch.cuda.is_available())\n","print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n"],"metadata":{"id":"EpZmYS2shlDA","executionInfo":{"status":"ok","timestamp":1742960225664,"user_tz":240,"elapsed":3900,"user":{"displayName":"vanellsa acha","userId":"11413626297686659354"}},"outputId":"2794f294-7c8c-490b-fdd9-aed07248be98","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU available: True\n","GPU name: NVIDIA A100-SXM4-40GB\n"]}]}]}